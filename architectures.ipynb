{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPApI/3Fiy3meCXc4QMVKmx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/architectures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architectures\n",
        "This file contains the description of the main architectures used within this project. Below we provide a detailed description of them.\n",
        "\n",
        "First we describe models that were used for Q-values predictions:\n",
        "\n",
        "    - DGN\n",
        "    - DQN\n",
        "    - Comm_net,\n",
        "    \n",
        "then we go over the methods that were used to aggregate hidden graph representations:\n",
        "\n",
        "    - SUM\n",
        "    - GCN\n",
        "\n",
        "and lastly we describe the **NetMon** class that was originally provided by the authors.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DoKHKB4X6eUI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnJd_LjT7e_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-values prediction\n",
        "something about..\n"
      ],
      "metadata": {
        "id": "uAEZJkWq7eS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classical MLP\n"
      ],
      "metadata": {
        "id": "0WbJtPp-eNmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    This is the underlying module for all used models within this work.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, mlp_units, activation_fn, activation_on_output = True):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.activation = activation_fn\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "\n",
        "        self.linear_layers = nn.ModuleList() # Storage for L layers\n",
        "        previous_units = in_features\n",
        "\n",
        "        # Transform units into a list\n",
        "        if isinstance(mlp_units, int):\n",
        "            mlp_units = [mlp_units]\n",
        "\n",
        "        # Create a chain of layers\n",
        "        for units in mlp_units:\n",
        "            self.linear_layers.append(nn.Linear(previous_units, units))\n",
        "            previous_units = units\n",
        "\n",
        "        self.out_features = previous_units\n",
        "        self.activation_on_ouput = activation_on_output\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Inter layers\n",
        "        for module in self.linear_layers[:-1]:\n",
        "            x = module(x)\n",
        "            if self.activation is not None:\n",
        "                x = self.activation(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Pass through the last layer\n",
        "        x = self.linear_layers[-1](x)\n",
        "        if self.activation_on_ouput:\n",
        "            x = self.activation(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "FelJGUB_gbhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention model"
      ],
      "metadata": {
        "id": "eunw7-MAgj1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttModel(nn.Module):\n",
        "    \"\"\"\n",
        "        Basic attention model with with masking and scaling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, k_features, v_features, out_features, num_heads, activation_fn, vkq_activation_fn):\n",
        "        super(AttModel, self).__init__()\n",
        "\n",
        "\n",
        "        self.k_features = k_features\n",
        "        self.v_features = v_features\n",
        "        self.num_heads = num_heads      # Number of attention heads\n",
        "\n",
        "        self.fc_v = nn.Linear(in_features, v_features * num_heads)  # Transforming input features into Values for attention\n",
        "        self.fc_k = nn.Linear(in_features, k_features * num_heads)  # Transforming input features into Keys for attention\n",
        "        self.fc_q = nn.Linear(in_features, k_features * num_heads)  # Transforming input values into Queries for attention\n",
        "\n",
        "        self.fc_out = nn.Linear(v_features * num_heads, out_features)   # Transforms the outputs from all attention heads into output dimension\n",
        "\n",
        "        self.activation = activation_fn\n",
        "        self.vkq_activation = vkq_activation_fn     # Activation function that can be applied into Values, Keys, Queries\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        Defining the scaling factor for attention as 1/ sqrt(d_k), this is the same as the publishing paper \"Attention is All You Need\".\n",
        "        This is done for the purpose of reducing the gradient so it does not become too large. Later you will see that without it, the dot product\n",
        "        would grow too large without the scaling\n",
        "        \"\"\"\n",
        "        self.attention_scale = 1 / (k_features **0.5)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, x, mask):\n",
        "        batch_size, num_agents = x.shape[0], x.shape[1]\n",
        "\n",
        "        \"\"\"\n",
        "        The code below does the following:\n",
        "            - a linear mapping is applied on the inputs to obtain Values, Keys, Queries\n",
        "            - the Values, Keys, Queries are then reshaped to separate the different attention heads of the model\n",
        "            :reshape: will result in (batch_size, num_agents, num_heads, features_per_head)\n",
        "\n",
        "        Visual representation:\n",
        "            Input x\n",
        "            |\n",
        "            [Linear Layers] -> V, Q, K\n",
        "            |\n",
        "            [Optional Activation] (vkq_activation_fn)\n",
        "            |\n",
        "            [Reshape for Multi-Head]\n",
        "            |\n",
        "            [Transpose for Heads]\n",
        "            |\n",
        "            [Compute Attention Weights (Dot Product, Scale, Mask, Softmax)]\n",
        "            |\n",
        "            [Apply Attention to Values]\n",
        "            |\n",
        "            [Skip Connection]\n",
        "            |\n",
        "            [Transpose and Concatenate Heads]\n",
        "            |\n",
        "            [Final Linear Layer and Activation]\n",
        "            |\n",
        "            Output\n",
        "        \"\"\"\n",
        "\n",
        "        v = self.fc_v(x).view(batch_size, num_agents, self.num_heads, self.v_features)\n",
        "        q = self.fc_q(x).view(batch_size, num_agents, self.num_heads, self.k_features)\n",
        "        k = self.fc_k(x).view(batch_size, num_agents, self.num_heads, self.k_features)\n",
        "\n",
        "        if self.vkq_activation is not None:\n",
        "            v = self.vkq_activation(v)\n",
        "            q = self.vkq_activation(q)\n",
        "            k = self.vkq_activation(k)\n",
        "\n",
        "        # We rearrange the tensors to shape (batch_size, num_heads, num_agents, features_per_head)\n",
        "        # This is done so we can perform batch multiplication over the batch size and heads\n",
        "        q, k, v = q.transpose(1,2), k.transpose(1,2), v.transpose(1,2)\n",
        "\n",
        "        # Add head axis (we are keeping the same mask for all attention heads)\n",
        "        mask = mask.unsqueeze(1)    # (batch_size, 1, num_agents, num_agents) (1,1,20,20)\n",
        "\n",
        "        \"\"\"\n",
        "        The attention is calculated as a dot product of all queries with all keys,\n",
        "            while scaling it with the attention scale so it does not explode.\n",
        "            - q is of shape             (batch_size, num_heads, num_agents, features_per_head)\n",
        "            - k transposed is of shape  (batch_size, num_heads, features_per_head, num_agents)\n",
        "            - the multiplication result is of shape (batch_size, num_heads, num_agents, num_agents)\n",
        "        :masked_fill sets positions where mask == 0 to a large negative value - removes them from the attention computation practically\n",
        "        \"\"\"\n",
        "\n",
        "        att_weights = torch.matmul(q, k.transpose(2, 3)) * self.attention_scale\n",
        "        att = att_weights.masked_fill(mask==0, -1e9)\n",
        "        att = F.softmax(att, dim=-1)    # Softmax is applied along the last dimension to obtain normalized attention probabilities\n",
        "        att = self.dropout(att)\n",
        "\n",
        "        # Now we combine the Values with respect to the attention we just computed\n",
        "        \"\"\"\n",
        "            - att is of shape (batch_size, num_heads, num_agents, num_agents)\n",
        "            - v is of shape (batch_size, num_heads, num_agents, v_features)\n",
        "            - the multiplication result is of shape (batch_size, num_heads, num_agents, v_features)\n",
        "        \"\"\"\n",
        "        out = torch.matmul(att, v)\n",
        "\n",
        "        # We add a skip connection\n",
        "        out  = torch.add(out, v)    # This additionally promotes gradient flow and mitigates vanishing gradient\n",
        "\n",
        "        # Now \"remove\" the transpose and concatenate all heads together\n",
        "        \"\"\"\n",
        "            - out is of shape (batch_size, num_heads, num_agents, v_features)\n",
        "            - out after transpose is of shape (batch_size, num_agents, num_heads, v_features)\n",
        "            - contiguous() ensures that the tensor is stored in a contiguous chunk of memory so that the reshape for view can happen\n",
        "            - view is used to reshape the tensor to (batch_size, num_agents, v_features), therefore, we flatten the last two dimensions\n",
        "                into a single one (num_heads * v_features)\n",
        "            - final out is of shape  (batch_size, num_agents, num_heads * v_features)\n",
        "        \"\"\"\n",
        "\n",
        "        out = out.transpose(1,2).contiguous().view(batch_size, num_agents, -1)\n",
        "        out = self.activation(self.fc_out(out)) # Linear map into a desired feature dimension\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out, att_weights"
      ],
      "metadata": {
        "id": "nrzRbyNXgYj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DGN\n",
        "what is it, how it works, provide that code"
      ],
      "metadata": {
        "id": "F6p203xn7mIg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8yXnKot6dTF"
      },
      "outputs": [],
      "source": [
        "class DGN(nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, mlp_units, num_actions, num_heads, num_attention_layers, activation_fn, kv_values):\n",
        "        super(DGN, self).__init__()\n",
        "\n",
        "        self.encoder = MLP(in_features, mlp_units, activation_fn)\n",
        "        self.att_layers = nn.ModuleList()\n",
        "        hidden_features = self.encoder.out_features\n",
        "\n",
        "        print(\"In features of DGN: \", in_features)\n",
        "        print(\"MLP units are: \", mlp_units)\n",
        "\n",
        "        for _ in range(num_attention_layers):\n",
        "            self.att_layers.append(\n",
        "                AttModel(hidden_features, kv_values, kv_values, hidden_features, num_heads, activation_fn, activation_fn)\n",
        "                                   )\n",
        "\n",
        "        self.q_net = Q_Net(hidden_features * (num_attention_layers + 1), num_actions)\n",
        "\n",
        "        self.att_weights = []\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Additional comment to the function:\n",
        "            - each attention layer refines the representation h by focusing on relevant parts of the input\n",
        "            - by concatenating the representations the feature set for the Q-network is enhanced, consequently making more informed decisions\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        h = self.encoder(x)     # Encodes the input featuers, has a shape of (batch_size, num_agents, hidden_features)\n",
        "        q_input = h     # Initialize the q_input with encoded features\n",
        "        self.att_weights.clear()    # Ensuring that attention weights from previous forward passes do not accumulate\n",
        "\n",
        "        for attention_layer in self.att_layers:\n",
        "            h, att_weight = attention_layer(h, mask)\n",
        "            self.att_weights.append(att_weight)\n",
        "\n",
        "            # Concatenation of outputs\n",
        "            q_input = torch.cat((q_input, h), dim=-1)\n",
        "\n",
        "        # Final q_input is of shape (batch_size, num_agents, hidden_features * (num_attention_layers +1))\n",
        "        q = self.q_net(q_input)\n",
        "\n",
        "        return q    # is of shape (batch_size, num_agents, num_actions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DQN\n",
        "\n",
        "same"
      ],
      "metadata": {
        "id": "xbzlC1hA7sGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    Introduces simple Deep Feed Forward Neural Network( = MLP) as the encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, mlp_units, num_actions, activation_fn):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.encoder = MLP(in_features, mlp_units, activation_fn)   # Encodes incoming features\n",
        "        self.q_net = Q_Net(self.encoder.out_features, num_actions)  # Outputs Q-values\n",
        "        self.activation = activation_fn\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        batch, agent, features = x.shape\n",
        "        h = self.encoder(x)\n",
        "        q = self.q_net(h)\n",
        "        return q\n"
      ],
      "metadata": {
        "id": "Sbs8yo5R72kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comm_net\n",
        "same"
      ],
      "metadata": {
        "id": "5AvA-UXx73H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNR(nn.Module):\n",
        "    \"\"\"\n",
        "    Recurrent DQN with an lstm cell.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, mlp_units, num_actions, activation_fn):\n",
        "        super(DQNR, self).__init__()\n",
        "        self.encoder = MLP(in_features, mlp_units, activation_fn)\n",
        "        self.lstm = nn.LSTMCell(\n",
        "            input_size=self.encoder.out_features, hidden_size=self.encoder.out_features\n",
        "        )\n",
        "        self.state = None\n",
        "        self.q_net = Q_Net(self.encoder.out_features, num_actions)\n",
        "\n",
        "    def get_state_len(self):\n",
        "        return 2 * self.lstm.hidden_size\n",
        "\n",
        "    def _state_reshape_in(self, batch_size, n_agents):\n",
        "        \"\"\"\n",
        "        Reshapes the state of shape\n",
        "            (batch_size, n_agents, self.get_state_len())\n",
        "        to shape\n",
        "            (2, batch_size * n_agents, hidden_size).\n",
        "\n",
        "        :param batch_size: the batch size\n",
        "        :param n_agents: the number of agents\n",
        "        \"\"\"\n",
        "        self.state = (\n",
        "            self.state.reshape(\n",
        "                batch_size * n_agents,\n",
        "                2,\n",
        "                self.lstm.hidden_size,\n",
        "            )\n",
        "            .transpose(0, 1)\n",
        "            .contiguous()\n",
        "        )\n",
        "\n",
        "    def _state_reshape_out(self, batch_size, n_agents):\n",
        "        \"\"\"\n",
        "        Reshapes the state of shape\n",
        "            (2, batch_size * n_agents, hidden_size)\n",
        "        to shape\n",
        "            (batch_size, n_agents, self.get_state_len()).\n",
        "\n",
        "        :param batch_size: the batch size\n",
        "        :param n_agents: the number of agents\n",
        "        \"\"\"\n",
        "        self.state = self.state.transpose(0, 1).reshape(batch_size, n_agents, -1)\n",
        "\n",
        "    def _lstm_forward(self, x, reshape_state=True):\n",
        "        \"\"\"\n",
        "        A single lstm forward pass\n",
        "\n",
        "        :param x: Cell input\n",
        "        :param reshape_state: reshape the state to and from (batch_size, n_agents, -1)\n",
        "        \"\"\"\n",
        "        batch_size, n_agents, feature_dim = x.shape\n",
        "        # combine agent and batch dimension\n",
        "        x = x.view(batch_size * n_agents, -1)\n",
        "\n",
        "        if self.state is None:\n",
        "            lstm_hidden_state, lstm_cell_state = self.lstm(x)\n",
        "        else:\n",
        "            if reshape_state:\n",
        "                self._state_reshape_in(batch_size, n_agents)\n",
        "            lstm_hidden_state, lstm_cell_state = self.lstm(\n",
        "                x, (self.state[0], self.state[1])\n",
        "            )\n",
        "\n",
        "        self.state = torch.stack((lstm_hidden_state, lstm_cell_state))\n",
        "        x = lstm_hidden_state\n",
        "\n",
        "        # undo combine\n",
        "        x = x.view(batch_size, n_agents, -1)\n",
        "        if reshape_state:\n",
        "            self._state_reshape_out(batch_size, n_agents)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        h = self.encoder(x)\n",
        "        h = self._lstm_forward(h)\n",
        "        return self.q_net(h)\n",
        "\n",
        "\n",
        "class CommNet(DQNR):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features,\n",
        "        mlp_units,\n",
        "        num_actions,\n",
        "        comm_rounds,\n",
        "        activation_fn,\n",
        "    ):\n",
        "        super().__init__(in_features, mlp_units, num_actions, activation_fn)\n",
        "        assert comm_rounds >= 0\n",
        "        self.comm_rounds = comm_rounds\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        batch_size, n_agents, feature_dim = x.shape\n",
        "        h = self.encoder(x)\n",
        "\n",
        "        # manually reshape state\n",
        "        if self.state is not None:\n",
        "            self._state_reshape_in(batch_size, n_agents)\n",
        "\n",
        "        h = self._lstm_forward(h, reshape_state=False)\n",
        "\n",
        "        # explicitly exclude self-communication from mask\n",
        "        mask = mask * ~torch.eye(n_agents, dtype=bool, device=x.device).unsqueeze(0)\n",
        "\n",
        "        for _ in range(self.comm_rounds):\n",
        "            # combine hidden state h according to mask\n",
        "            # first add up hidden states according to mask\n",
        "            #    h has dimensions (batch, agents, features)\n",
        "            #    and mask has dimensions (batch, agents, neighbors)\n",
        "            #    => we have to transpose the mask to aggregate over all neighbors\n",
        "            c = torch.bmm(h.transpose(1, 2), mask.transpose(1, 2)).transpose(1, 2)\n",
        "            # then normalize according to number of neighbors per agent\n",
        "            c = c / torch.clamp(mask.sum(dim=-1).unsqueeze(-1), min=1)\n",
        "\n",
        "            # skip connection for hidden state and communication\n",
        "            h = h + c\n",
        "            # use new hidden state\n",
        "            self.state[0] = h.view(batch_size * n_agents, -1)\n",
        "\n",
        "            # pass through forward module\n",
        "            h = self._lstm_forward(h, reshape_state=False)\n",
        "\n",
        "        # manually reshape state in the end\n",
        "        self._state_reshape_out(batch_size, n_agents)\n",
        "        return self.q_net(h)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uBT6HNhl78SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## State aggregation\n",
        "\n",
        "...."
      ],
      "metadata": {
        "id": "AuP5aqqV786_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SUM\n"
      ],
      "metadata": {
        "id": "FUlFeCs-8BC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleAggregation(nn.Module):\n",
        "    def __init__(self, agg: str, mask_eye: bool) -> None:\n",
        "        super().__init__()\n",
        "        self.agg = agg\n",
        "        assert self.agg == \"mean\" or self.agg == \"sum\"\n",
        "        self.mask_eye = mask_eye\n",
        "\n",
        "    def forward(self, node_features, node_adjacency):\n",
        "        if self.mask_eye:\n",
        "            node_adjacency = node_adjacency * ~(\n",
        "                torch.eye(\n",
        "                    node_adjacency.shape[1],\n",
        "                    node_adjacency.shape[1],\n",
        "                    device=node_adjacency.device,\n",
        "                )\n",
        "                .repeat(node_adjacency.shape[0], 1, 1)\n",
        "                .bool()\n",
        "            )\n",
        "        feature_sum = torch.bmm(node_adjacency, node_features)\n",
        "        if self.agg == \"sum\":\n",
        "            return feature_sum\n",
        "        if self.agg == \"mean\":\n",
        "            num_neighbors = torch.clamp(node_adjacency.sum(dim=-1), min=1).unsqueeze(-1)\n",
        "            return feature_sum / num_neighbors\n"
      ],
      "metadata": {
        "id": "NgTugJxZ8EFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GCN\n",
        "GCN is a graph convolutional operator that handles Message Passing phase within the GNN. Implementation is available at [GCN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv) within the pytorch_geometric library that specializes on GNNs.\n",
        "\n",
        "GCN is based on the spectral approximations of convolutional"
      ],
      "metadata": {
        "id": "b8iKtdFe8EjR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0H2R02_E8KJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NetMon"
      ],
      "metadata": {
        "id": "YeJELh3k8Hll"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D9qyiyhF8VP7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}