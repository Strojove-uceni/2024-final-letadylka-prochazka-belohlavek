{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE OF CONTENTS**"
      ],
      "metadata": {
        "id": "xgl4XoSJHuxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Introduction](#scrollTo=Mey2sVO3w06m)\n",
        "\n",
        ">>[Abstract](#scrollTo=Mey2sVO3w06m)\n",
        "\n",
        ">>[How to Run the Project](#scrollTo=Mey2sVO3w06m)\n",
        "\n",
        ">>>[Disclamer](#scrollTo=ZDfbtUw36j_1)\n",
        "\n",
        ">[Reinforcement Learning](#scrollTo=ZDfbtUw36j_1)\n",
        "\n",
        ">>[Problems of RL](#scrollTo=ZDfbtUw36j_1)\n",
        "\n",
        ">>>[Idea behind the approach we use](#scrollTo=ZDfbtUw36j_1)\n",
        "\n",
        ">>[Replay Buffer](#scrollTo=ZDfbtUw36j_1)\n",
        "\n",
        ">>>>[Importance for RNNS](#scrollTo=ZDfbtUw36j_1)\n",
        "\n",
        ">>>>[Prioritized Experience Replay](#scrollTo=ZDfbtUw36j_1)\n",
        "\n",
        ">>>>>[Sampling Mechanism](#scrollTo=ZDfbtUw36j_1)\n",
        "\n",
        ">[NN Components](#scrollTo=xdxNv9pODPz0)\n",
        "\n",
        ">>[Classical MLP](#scrollTo=Z4BoIqSPdCtb)\n",
        "\n",
        ">>[Attention Model](#scrollTo=HU4Z78Kv9hiW)\n",
        "\n",
        ">>[Q-values Prediction](#scrollTo=FN9jHalCI6UK)\n",
        "\n",
        ">>>[Q-Net](#scrollTo=65H4yFBgdJeh)\n",
        "\n",
        ">>>[DGN](#scrollTo=_s_7VNFm91w8)\n",
        "\n",
        ">>>[DQN](#scrollTo=G5-kiKLj98yv)\n",
        "\n",
        ">>>[CommNet](#scrollTo=VK8i3VvG-Bg-)\n",
        "\n",
        ">>>[Reward System: Detailed Description and Discussion](#scrollTo=f_ZqrL8K8zqq)\n",
        "\n",
        ">>>>[Reward Components](#scrollTo=f_ZqrL8K8zqq)\n",
        "\n",
        ">>>[Summary](#scrollTo=f_ZqrL8K8zqq)\n",
        "\n",
        ">>[State Aggregation](#scrollTo=AGSMkNTI-KIH)\n",
        "\n",
        ">>>[SUM](#scrollTo=AGSMkNTI-KIH)\n",
        "\n",
        ">>>[GCN](#scrollTo=7S-b66SX-TyK)\n",
        "\n",
        ">>[NetMon](#scrollTo=ojoJfBcX-Wrm)\n",
        "\n",
        ">>>[State management](#scrollTo=ojoJfBcX-Wrm)\n",
        "\n",
        ">[Multi GPU setups](#scrollTo=Pw68VqsjYacn)\n",
        "\n",
        ">>[Bi-GPU setup](#scrollTo=Pw68VqsjYacn)\n",
        "\n",
        ">>[Multi-GPU setup](#scrollTo=Pw68VqsjYacn)\n",
        "\n",
        ">[Selecting Parameters](#scrollTo=1Ei12632gg3Z)\n",
        "\n",
        ">>[Common Parameters in the Sweep](#scrollTo=1Ei12632gg3Z)\n",
        "\n",
        ">>[CommNet specific](#scrollTo=1Ei12632gg3Z)\n",
        "\n",
        ">>[DQN, DGN specific](#scrollTo=1Ei12632gg3Z)\n",
        "\n",
        ">[Advice for Parameter Selection](#scrollTo=OhkNKLaYlGRj)\n",
        "\n",
        ">>[CommNet settings](#scrollTo=OhkNKLaYlGRj)\n",
        "\n",
        ">>[DQN, DGN settings](#scrollTo=OhkNKLaYlGRj)\n",
        "\n",
        ">>[Aggregation Type](#scrollTo=OhkNKLaYlGRj)\n",
        "\n",
        ">>>[WANDB sweep for CommNet](#scrollTo=e1gjgCZZzhzm)\n",
        "\n",
        ">>>[Hyperparameters importance for CommNet](#scrollTo=ghDCmQ55xYYl)\n",
        "\n",
        ">>>[WANDB sweep for DQN vs. DGN](#scrollTo=TO3iriopz9P7)\n",
        "\n",
        ">>>[HYPERPARAMETERS IMPORTANCE FOR DGN and DQN](#scrollTo=ca4DwESCxrjT)\n",
        "\n",
        ">[Our Results](#scrollTo=S8nU4WQLwJI7)\n",
        "\n",
        ">>[Our Selected Best Performing Models](#scrollTo=S8nU4WQLwJI7)\n",
        "\n",
        ">>>[Key observations](#scrollTo=S8nU4WQLwJI7)\n",
        "\n",
        ">>>[Hyperparameter search](#scrollTo=S8nU4WQLwJI7)\n",
        "\n",
        ">>[Heatmaps](#scrollTo=K0VfxzH_jn4H)\n",
        "\n",
        ">>>[COMMNET](#scrollTo=8GQ6b3ibeK6K)\n",
        "\n",
        ">>>[DQN](#scrollTo=eNq4qi-Pjbwl)\n",
        "\n",
        ">>>[DGN](#scrollTo=xkXvwNuhkwSj)\n",
        "\n",
        ">>[Shortest Path Ratio](#scrollTo=MMiw4WCyk_W9)\n",
        "\n",
        ">>>[COMMNET](#scrollTo=-qaQ7fj_mNZJ)\n",
        "\n",
        ">>>[DQN](#scrollTo=E3XestfLmXa0)\n",
        "\n",
        ">[Conclusion](#scrollTo=buWgEb9M5rJd)\n",
        "\n",
        ">[References](#scrollTo=XI6rlOjqEeNN)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "Ud4C609CHs-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Authors: Michal Bělohlávek, Tomáš Procházka**\n",
        "\n",
        "# <u>**Introduction**</u>\n",
        "Welcome to the demo file, where you can run the project effortlessly and view the results and visualizations firsthand. While this provides an easy and pleasant way to experience our neural network, we strongly encourage anyone visiting this demo to run the project as intended, expand upon it, and enhance its capabilities.\n",
        "\n",
        "## **Abstract**\n",
        "This project was created and submitted as the final semester project for the Machine Learning 2 class at FNSPE CTU. It focuses on reinforcement learning for multiple agents controlled by a single neural network within a graph environment. The primary objective is to develop a neural network solution capable of efficiently navigating multiple planes across a fully connected graph, estimating the shortest paths while avoiding plane collisions.\n",
        "To facilitate the learning process, we implemented an enhanced version of the classical replay buffer, which samples experiences based on predicted future rewards. Additionally, we created a dense reward system to encourage traversal along longer paths and implemented node masking to enable generalization across graphs with diverse neighbourhoods. Lastly, we provide additional code for multi-GPU setups to enable faster inference times.\n",
        "\n",
        "## **How to Run the Project**\n",
        "For those who decide to download the project and run the training on their PC, please beware of the configurations. A basic setup is present in /data as demo_config.yaml and runs on CPU.\n",
        "\n",
        "Setting up capacity, minibatch_size or sequence_length too high may result in freezing the computer.\n",
        "\n",
        "Most hyperparameters may be changed in the config.yaml file. If you intend to do your own sweeps on weights and biases, we have also uploaded a version of the main file ```wandb_main.py``` that supports sweep configuration.\n",
        "\n",
        "If you however decide to only run the project in this demo file, note that any pre-trained models are too large to upload to the GitHub repo directly, so the training will be done from scratch here. The training will use the ```demo_confing.yaml``` with small number of steps and generally \"low\" settings, so taht the training can be completed in reasonable amount of time. Therefore, one should expect very poor results compared to the results we present at the end of this notebook. Note that we need to install specific versions of many libraries that are compatible, this may take a while.\n",
        "\n",
        "The Runtime may encounter an error with tensorflow, simply restart Runtime and run the following cells and training will start. The training takes about 5 minutes. Sometimes Google Colab fails to connect to GitHub. The error you may encounter is that it fails to find the directory. To solve this, simply keep deleting the runtime until it works. If you encounter any errors about pip dependency, ignore it and proceed.\n",
        "\n",
        "**To see the training and results, simply run the following code boxes.**\n"
      ],
      "metadata": {
        "id": "Mey2sVO3w06m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek"
      ],
      "metadata": {
        "id": "9mdfd6guYlQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/2024-final-letadylka-prochazka-belohlavek/\n",
        "!pip install -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt"
      ],
      "metadata": {
        "id": "XVMOpK9Tgo63",
        "outputId": "374b575d-5545-456b-c61c-e4ef2fab4e1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/2024-final-letadylka-prochazka-belohlavek\n",
            "Collecting gymnasium==1.0.0 (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 1))\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting matplotlib==3.9.3 (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2))\n",
            "  Downloading matplotlib-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: networkx==3.4.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 3)) (3.4.2)\n",
            "Collecting numpy==2.2.0 (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 4))\n",
            "  Downloading numpy-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==2.2.3 (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 5))\n",
            "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 6)) (6.0.2)\n",
            "Collecting torch==2.5.0 (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torch_geometric==2.6.1 (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9))\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.67.0 (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 10))\n",
            "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb==0.19.0 (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11))\n",
            "  Downloading wandb-0.19.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.0.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.0.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 1)) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium==1.0.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 1))\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2)) (4.55.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2)) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 5)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 5)) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8)) (3.16.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8)) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8)) (1.13.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (3.11.10)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (5.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (2.32.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (4.25.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (2.10.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (75.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (4.0.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8)) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (5.0.1)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.19.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, triton, tqdm, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gymnasium, wandb, nvidia-cusolver-cu12, matplotlib, torch_geometric, torch\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.6\n",
            "    Uninstalling tqdm-4.66.6:\n",
            "      Successfully uninstalled tqdm-4.66.6\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: wandb\n",
            "    Found existing installation: wandb 0.18.7\n",
            "    Uninstalling wandb-0.18.7:\n",
            "      Successfully uninstalled wandb-0.18.7\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.8.0\n",
            "    Uninstalling matplotlib-3.8.0:\n",
            "      Successfully uninstalled matplotlib-3.8.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.2.0 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "langchain 0.3.10 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.0 which is incompatible.\n",
            "pytensor 2.26.4 requires numpy<2,>=1.17.0, but you have numpy 2.2.0 which is incompatible.\n",
            "tensorflow 2.17.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.2.0 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.0 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.5.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed farama-notifications-0.0.4 gymnasium-1.0.0 matplotlib-3.9.3 numpy-2.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pandas-2.2.3 torch-2.5.0 torch_geometric-2.6.1 tqdm-4.67.0 triton-3.1.0 wandb-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow, pandas\n",
        "!pip install numpy==1.26.4, tensorboard==2.18.0, pandas==2.2.2\n",
        "!conda install -c conda-forge cudatoolkit cudnn"
      ],
      "metadata": {
        "id": "SVFsINSti3Vg",
        "outputId": "7b259d77-134e-42d9-d41a-19491daa54fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'tensorflow,': Expected end or semicolon (after name and no valid version specifier)\n",
            "    tensorflow,\n",
            "              ^\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard==2.18.0\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pandas==2.2.2\n",
            "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (1.68.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (3.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2024.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard==2.18.0) (3.0.2)\n",
            "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, tensorboard, pandas\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.0\n",
            "    Uninstalling numpy-2.2.0:\n",
            "      Successfully uninstalled numpy-2.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.3\n",
            "    Uninstalling pandas-2.2.3:\n",
            "      Successfully uninstalled pandas-2.2.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires tensorboard<2.18,>=2.17, but you have tensorboard 2.18.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 pandas-2.2.2 tensorboard-2.18.0\n",
            "/bin/bash: line 1: conda: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd  /content/2024-final-letadylka-prochazka-belohlavek\n",
        "!python /content/2024-final-letadylka-prochazka-belohlavek/src/main.py --demo_config data/demo_config.yaml"
      ],
      "metadata": {
        "id": "cIBJLw9yglGC",
        "outputId": "ed64b6a7-827a-4a23-80b4-cdd30f3ef98f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/2024-final-letadylka-prochazka-belohlavek\n",
            "2024-12-13 14:24:19.342974: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-13 14:24:19.371717: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-13 14:24:19.379803: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-13 14:24:19.404203: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-13 14:24:21.480160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "seed for the buffer is:  595292149\n",
            "/usr/local/lib/python3.10/dist-packages/networkx/utils/backends.py:1355: FutureWarning: \n",
            "\n",
            "shortest_path will return an iterator that yields\n",
            "(node, path) pairs instead of a dictionary when source\n",
            "and target are unspecified beginning in version 3.5\n",
            "\n",
            "To keep the current behavior, use:\n",
            "\n",
            "\tdict(nx.shortest_path(G))\n",
            "  return self._call_with_backend(backend_name, args, kwargs)\n",
            "Sizes before netmon:\n",
            "Agent observation size:  1458\n",
            "Node observation size:  1200\n",
            "Sizes after netmon:\n",
            "Node state size: 256\n",
            "Agent observation size with netmon: 1714\n",
            "Node auxiliary size: 118\n",
            "In features are:  512\n",
            "runs/Dec13_14-24-24_954e8eefd19a_R2_netmon\n",
            "Training with parameters\n",
            "Model type: DQN\n",
            "NetMon Module\n",
            "+----------------------------------+---------------------------------------------+------------------------+-----------+\n",
            "| Layer                            | Input Shape                                 | Output Shape           | #Param    |\n",
            "|----------------------------------+---------------------------------------------+------------------------+-----------|\n",
            "| NetMon                           | [1, 118, 1200], [1, 118, 118], [1, 118, 10] | [1, 10, 256]           | 2,084,480 |\n",
            "| ├─(encode)MLP                    | [118, 1200]                                 | [118, 128]             | 1,820,288 |\n",
            "| │    └─(dropout)Dropout          | [118, 1024]                                 | [118, 1024]            | --        |\n",
            "| │    └─(linear_layers)ModuleList | --                                          | --                     | 1,820,288 |\n",
            "| │    │    └─(0)Linear            | [118, 1200]                                 | [118, 1024]            | 1,229,824 |\n",
            "| │    │    └─(1)Linear            | [118, 1024]                                 | [118, 512]             | 524,800   |\n",
            "| │    │    └─(2)Linear            | [118, 512]                                  | [118, 128]             | 65,664    |\n",
            "| ├─(aggregate)SimpleAggregation   | [1, 118, 128], [1, 118, 118]                | [1, 118, 128]          | --        |\n",
            "| ├─(rnn_obs)LSTMCell              | [118, 128]                                  | [118, 128], [118, 128] | 132,096   |\n",
            "| ├─(rnn_update)LSTMCell           | [118, 128]                                  | [118, 128], [118, 128] | 132,096   |\n",
            "+----------------------------------+---------------------------------------------+------------------------+-----------+\n",
            "> Aggregation Type: sum\n",
            "> RNN Type: lstm\n",
            "> Carryover: True\n",
            "> Iterations: 4\n",
            "> Readout: local + global agg\n",
            "Routing environment with parameters\n",
            "> Network: 118 nodes\n",
            "> Number of planes: 10\n",
            "> Environment variant: WITH_K_NEIGHBORS\n",
            "> Number of considered neighbors (k): 4\n",
            "> Action mask: False            \n",
            "▲ environment is wrapped with NetMon (graph obs)\n",
            "Episode: 1  step: 0k  reward: 0.75  delays: 90.91  delays_arrived: 82.00  spr: 28.26  looped: 2.92  throughput: 0.01  dropped: 0.00  blocked: 0.00 | BEST\n",
            "Episode: 2  step: 0k  reward: 0.85  delays: 83.33  delays_arrived: 58.50  spr: 18.45  looped: 3.12  throughput: 0.02  dropped: 0.00  blocked: 0.00 | BEST\n",
            "Episode: 3  step: 0k  reward: 0.85  delays: 83.33  delays_arrived: 51.00  spr: 10.99  looped: 2.92  throughput: 0.02  dropped: 0.00  blocked: 0.00  loss_aux: 170.29  q_values: 0.75  q_target: 0.95  loss: 6.32\n",
            "Episode: 4  step: 0k  reward: 0.96  delays: 90.91  delays_arrived: 38.00  spr: 25.00  looped: 3.09  throughput: 0.01  dropped: 0.00  blocked: 0.00  loss_aux: 68.60  q_values: 0.94  q_target: 1.02  loss: 4.50 | BEST\n",
            "Episode: 5  step: 0k  reward: 0.80  delays: 90.91  delays_arrived: 9.00  spr: 3.23  looped: 3.74  throughput: 0.01  dropped: 0.00  blocked: 0.00  loss_aux: 51.29  q_values: 1.05  q_target: 1.05  loss: 4.43\n",
            "Episode: 6  step: 1k  reward: 1.05  delays: 83.33  delays_arrived: 31.00  spr: 11.85  looped: 3.08  throughput: 0.02  dropped: 0.00  blocked: 0.02  loss_aux: 49.59  q_values: 1.04  q_target: 1.07  loss: 4.39 | BEST\n",
            "Episode: 7  step: 1k  reward: 1.14  delays: 71.43  delays_arrived: 43.25  spr: 12.64  looped: 2.68  throughput: 0.04  dropped: 0.00  blocked: 0.00  loss_aux: 44.22  q_values: 1.20  q_target: 1.35  loss: 4.83 | BEST\n",
            "Episode: 8  step: 1k  reward: 0.99  delays: 83.33  delays_arrived: 71.00  spr: 24.52  looped: 2.82  throughput: 0.02  dropped: 0.00  blocked: 0.01  loss_aux: 34.76  q_values: 1.37  q_target: 1.36  loss: 4.68\n",
            "Episode: 9  step: 1k  reward: 0.80  delays: 71.43  delays_arrived: 59.75  spr: 24.93  looped: 3.18  throughput: 0.04  dropped: 0.00  blocked: 0.01  loss_aux: 19.24  q_values: 1.46  q_target: 1.62  loss: 4.78\n",
            "Episode: 10  step: 1k  reward: 0.79  delays: 90.91  delays_arrived: 53.00  spr: 11.16  looped: 3.01  throughput: 0.01  dropped: 0.00  blocked: 0.03  loss_aux: 14.16  q_values: 1.51  q_target: 1.52  loss: 4.36\n",
            "Clean exit\n",
            "Performing evaluation:\n",
            "Plane  6  reached the target.\n",
            "Plane  8  reached the target.\n",
            "Plane  8  reached the target.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/2024-final-letadylka-prochazka-belohlavek/src/main.py\", line 602, in <module>\n",
            "    paths_to_save = env.save_paths()\n",
            "  File \"/content/2024-final-letadylka-prochazka-belohlavek/src/wrapper.py\", line 29, in __getattr__\n",
            "    return getattr(self.env, name)  # If not found in the Netmon, look into the environment\n",
            "AttributeError: 'Routing' object has no attribute 'save_paths'\n",
            "An exception was raised during evaluation (see above).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- # Overview of the Used Machine Learning Techniques -->\n",
        "<span><font color=\"green;\">\n",
        "###Disclamer\n",
        "\n",
        "**The following code is used as an illustration only, to see the functionalities of the code directly, visit the /src file on our GitHub repo. This approach was taken because it is not feasible to copy the whole code into Colab.**\n",
        "</font></span>\n",
        "\n",
        "(Or maybe it would be feasible but that would be an extreme violation of our hard work.)\n",
        "\n",
        "# <u>**Reinforcement Learning**</u>\n",
        "**Reinforcement Learning (RL)** is a powerful machine learning paradigm where agents learn to make decisions by interacting with some environment. Unlike Supervise Learning, where the model is trained on labeled data, RL agengts learn optimal behaviors through trial and error, guided by feedback in the form of rewards.\n",
        "\n",
        "**Who/What is an agent/environment?**\n",
        "\n",
        "- Agent is someone who makes decisions while interacting with the environment to achieve predefined goals.\n",
        "- Environment is some external system with which the agent interacts. It provides something that we call **observations** and rewards based on the agent's actions.\n",
        "\n",
        "**What is an observation?**\n",
        "\n",
        "Observation represents the current state/situation of the environment which is perceived by the agent. It contains necessary information required for the agent to make a decision.\n",
        "\n",
        "**What are actions?**\n",
        "\n",
        "Actions are all the possible decisions the agent can take in a given state.\n",
        "\n",
        "\n",
        "**What is a reward?**\n",
        "\n",
        "Reward is a single number representing a feedback signal received by the agent after performing an action. It indicates the immediate benefit of that action.\n",
        "\n",
        "**What is an experience?**\n",
        "\n",
        "Experience compromises tuples - (state, action, reward, next state) that the agent accumulates over time while interacting with the environment. These experiences are crucial for learning and are stored in a replay buffer for training purposes.\n",
        "\n",
        "**What is a policy?**\n",
        "\n",
        "Policy is a strategy that the agent employs to decide actions based on the current state.\n",
        "\n",
        "**What is a Q-function?**\n",
        "\n",
        "Q-function is representing the expected cumulative reward of taking a particular action in a given state.\n",
        "\n",
        "## <u>**Problems of RL**</u>\n",
        "In systems where **several agents** are present (**Multi-Agent RL**) few problems arise with how everything is pipelined.\n",
        "\n",
        "RL is often divided into two categories. **Centralized** approach and **Decentralized** approach. **Centralized** approach often involves a **single 'controller'/coordinator** that has access to all agents' information and makes decisions for all of the agents. **Decentralized** version enables each agent to make decesions **independently**, while relying on local information and limited communication. Both of these approaches run into several problems. The former allows for the best decision making, but does not scale to large graphs. On the other hand, the latter does not have access to enough information, making it less reactive.\n",
        "\n",
        "### **Idea behind the approach we use**\n",
        "The observation space is expanded with learned graph (**environment**) observations that leverage recurrent message passing. This approach keeps the agents reactive and do not have to grather infromatin about the whole graph before taking an action.\n",
        "\n",
        "**This idea is not ours. We do not claim it, it was taken from [here](https://github.com/jw3il/graph-marl).**\n",
        "\n",
        "\n",
        "## <u>**Replay Buffer**</u>\n",
        "Our work prouds itself amongst other things on the advancement of a random **Replay Buffer** (within ```replay_buffer.py```) that significantly improved the prediction of paths that lead to future reward. A replay buffer is a key component in Reinforcement Learning that stores past experiences. This storage mechanism is essential for training models, especially those utilizing **RNN**s (**Recurrent Neural Networks**), which rely on sequential dependencies to function effectively.\n",
        "\n",
        "#### **Importance for RNNS**\n",
        "For RNNs, replay buffers play a vital role by allowing agents to learn from diverse trajectories while maintaining temporal coherence. Unlike traditional models that might sample individual transitions randomly, our approach involves sampling batches of sequences. This method ensures that the RNN captures meaningful patterns over time, thereby improving its ability to model long-term dependencies and make more informed predictions.\n",
        "\n",
        "#### **Prioritized Experience Replay**\n",
        "To further enhance the effectiveness of the replay buffer, we implemented a **Prioritized Experience Replay** mechanism. This version of replay buffer samples the batch sequences based on maximizing the **temporal differnce**(**TD**) **error**, which is the mean square error between predicted future and immediate rewards. By prioritizing experiences with higher TD errors, the agent focuses more on learning from actions that have a significant impact on future rewards\n",
        "\n",
        "##### **Sampling Mechanism**\n",
        "The sequences are still sampled on random but the sampling is weighted by the probabilities based on **priority**. The sampling probability is calculated as follows:\n",
        "\n",
        "$$\\text{probability} = \\text{priority}^{\\alpha}, \\quad \\alpha \\in [0, 1]$$\n",
        "and then normalized to form a valid probability distribution\n",
        "$$\\text{probability} = \\frac{\\text{priority}}{\\sum \\text{probability}}.$$\n",
        "\n",
        "In addition, there is a system of weighing the TD errores to prevent over-prioritizing some sampled indices. This is done through scaling parameter $\\beta$ that gradually increases from 0.4 to 1.0 during training, allowing the model to transition smoothly into using prioritized sequences.\n",
        "\n",
        "You can read more on this prioritized sampling in the referenced [repo](https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/rl/dqn/replay_buffer.py).\n"
      ],
      "metadata": {
        "id": "ZDfbtUw36j_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <u>**NN Components**</u>\n",
        "In the text below, we first describe models that were used for Q-values predictions:\n",
        "- **DGN**\n",
        "- **DQN**\n",
        "- **Comm_net**,\n",
        "    \n",
        "then we go over the methods that were used to aggregate hidden node states:\n",
        "\n",
        "- **SUM**\n",
        "- **GCN**\n",
        "\n",
        "and lastly we describe the **NetMon** class, that was originally provided by the authors."
      ],
      "metadata": {
        "id": "xdxNv9pODPz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Classical MLP**\n",
        "MLP is a feed forward network that passes the input thourgh many linear layers with activation functions. In our case, we used leaky-ReLU as the activation function. It is also possible to modify this setting to for example GeLU in the config.yaml file but we should points out that this may lead to the agent learning to take forbidden edges, that are subsequently masked leading to insufficient gradient flow. This approach has not been explored in this project. Dropout is included for regularization."
      ],
      "metadata": {
        "id": "Z4BoIqSPdCtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    This is the underlying module for all used models within this work.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, mlp_units, activation_fn, activation_on_output = True):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.activation = activation_fn\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "\n",
        "        self.linear_layers = nn.ModuleList() # Storage for L layers\n",
        "        previous_units = in_features\n",
        "\n",
        "        # Transform units into a list\n",
        "        if isinstance(mlp_units, int):\n",
        "            mlp_units = [mlp_units]\n",
        "\n",
        "        # Create a chain of layers\n",
        "        for units in mlp_units:\n",
        "            self.linear_layers.append(nn.Linear(previous_units, units))\n",
        "            previous_units = units\n",
        "\n",
        "        self.out_features = previous_units\n",
        "        self.activation_on_ouput = activation_on_output\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Inter layers\n",
        "        for module in self.linear_layers[:-1]:\n",
        "            x = module(x)\n",
        "            if self.activation is not None:\n",
        "                x = self.activation(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Pass through the last layer\n",
        "        x = self.linear_layers[-1](x)\n",
        "        if self.activation_on_ouput:\n",
        "            x = self.activation(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "7gDhEfU66-5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Attention Model**\n",
        "The AttModel class implements a multi-head attention mechanism that is inspired by \"Attention is All You Need\" paradigm. It utilizes the multiple attention head to capture different aspects of the input simultaneously. Scaling factors are applied to stabilize gradients during training.\n",
        "\n",
        "Moreover, masking is used to further specialize the input and the values calculated based on the input data and the state of the agents with the environment.\n",
        "\n"
      ],
      "metadata": {
        "id": "HU4Z78Kv9hiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttModel(nn.Module):\n",
        "    \"\"\"\n",
        "        Basic attention model with with masking and scaling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, k_features, v_features, out_features, num_heads, activation_fn, vkq_activation_fn):\n",
        "        super(AttModel, self).__init__()\n",
        "\n",
        "\n",
        "        self.k_features = k_features\n",
        "        self.v_features = v_features\n",
        "        self.num_heads = num_heads      # Number of attention heads\n",
        "\n",
        "        self.fc_v = nn.Linear(in_features, v_features * num_heads)  # Transforming input features into Values for attention\n",
        "        self.fc_k = nn.Linear(in_features, k_features * num_heads)  # Transforming input features into Keys for attention\n",
        "        self.fc_q = nn.Linear(in_features, k_features * num_heads)  # Transforming input values into Queries for attention\n",
        "\n",
        "        self.fc_out = nn.Linear(v_features * num_heads, out_features)   # Transforms the outputs from all attention heads into output dimension\n",
        "\n",
        "        self.activation = activation_fn\n",
        "        self.vkq_activation = vkq_activation_fn     # Activation function that can be applied into Values, Keys, Queries\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        Defining the scaling factor for attention as 1/ sqrt(d_k), this is the same as the publishing paper \"Attention is All You Need\".\n",
        "        This is done for the purpose of reducing the gradient so it does not become too large. Later you will see that without it, the dot product\n",
        "        would grow too large without the scaling.\n",
        "        \"\"\"\n",
        "        self.attention_scale = 1 / (k_features **0.5)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, x, mask):\n",
        "        batch_size, num_agents = x.shape[0], x.shape[1]\n",
        "\n",
        "        \"\"\"\n",
        "        The code below does the following:\n",
        "            - a linear mapping is applied on the inputs to obtain Values, Keys, Queries\n",
        "            - the Values, Keys, Queries are then reshaped to separate the different attention heads of the model\n",
        "            :reshape: will result in (batch_size, num_agents, num_heads, features_per_head)\n",
        "\n",
        "        Pipeline:\n",
        "                  Input x\n",
        "                    |\n",
        "              [Linear Layers] -> V, Q, K\n",
        "                    |\n",
        "            [Optional Activation] (vkq_activation_fn)\n",
        "                    |\n",
        "            [Reshape for Multi-Head]\n",
        "                    |\n",
        "            [Transpose for Heads]\n",
        "                    |\n",
        "            [Compute Attention Weights (Dot Product, Scale, Mask, Softmax)]\n",
        "                    |\n",
        "            [Apply Attention to Values]\n",
        "                    |\n",
        "            [Skip Connection]\n",
        "                    |\n",
        "            [Transpose and Concatenate Heads]\n",
        "                    |\n",
        "            [Final Linear Layer and Activation]\n",
        "                    |\n",
        "                  Output\n",
        "        \"\"\"\n",
        "\n",
        "        v = self.fc_v(x).view(batch_size, num_agents, self.num_heads, self.v_features)\n",
        "        q = self.fc_q(x).view(batch_size, num_agents, self.num_heads, self.k_features)\n",
        "        k = self.fc_k(x).view(batch_size, num_agents, self.num_heads, self.k_features)\n",
        "\n",
        "        if self.vkq_activation is not None:\n",
        "            v = self.vkq_activation(v)\n",
        "            q = self.vkq_activation(q)\n",
        "            k = self.vkq_activation(k)\n",
        "\n",
        "        # We rearrange the tensors to shape (batch_size, num_heads, num_agents, features_per_head)\n",
        "        # This is done so we can perform batch multiplication over the batch size and heads\n",
        "        q, k, v = q.transpose(1,2), k.transpose(1,2), v.transpose(1,2)\n",
        "\n",
        "        # Add head axis (we are keeping the same mask for all attention heads)\n",
        "        mask = mask.unsqueeze(1)    # (batch_size, 1, num_agents, num_agents) (1,1,20,20)\n",
        "\n",
        "        \"\"\"\n",
        "        The attention is calculated as a dot product of all queries with all keys,\n",
        "            while scaling it with the attention scale so it does not explode.\n",
        "            - q is of shape             (batch_size, num_heads, num_agents, features_per_head)\n",
        "            - k transposed is of shape  (batch_size, num_heads, features_per_head, num_agents)\n",
        "            - the multiplication result is of shape (batch_size, num_heads, num_agents, num_agents)\n",
        "        :masked_fill sets positions where mask == 0 to a large negative value - removes them from the attention computation practically\n",
        "        \"\"\"\n",
        "\n",
        "        att_weights = torch.matmul(q, k.transpose(2, 3)) * self.attention_scale\n",
        "        att = att_weights.masked_fill(mask==0, -1e9)\n",
        "        att = F.softmax(att, dim=-1)    # Softmax is applied along the last dimension to obtain normalized attention probabilities\n",
        "        att = self.dropout(att)\n",
        "\n",
        "        # Now we combine the Values with respect to the attention we just computed\n",
        "        \"\"\"\n",
        "            - att is of shape (batch_size, num_heads, num_agents, num_agents)\n",
        "            - v is of shape (batch_size, num_heads, num_agents, v_features)\n",
        "            - the multiplication result is of shape (batch_size, num_heads, num_agents, v_features)\n",
        "        \"\"\"\n",
        "        out = torch.matmul(att, v)\n",
        "\n",
        "        # We add a skip connection\n",
        "        out  = torch.add(out, v)    # This additionally promotes gradient flow and mitigates vanishing gradient\n",
        "\n",
        "        # Now \"remove\" the transpose and concatenate all heads together\n",
        "        \"\"\"\n",
        "            - out is of shape (batch_size, num_heads, num_agents, v_features)\n",
        "            - out after transpose is of shape (batch_size, num_agents, num_heads, v_features)\n",
        "            - contiguous() ensures that the tensor is stored in a contiguous chunk of memory so that the reshape for view can happen\n",
        "            - view is used to reshape the tensor to (batch_size, num_agents, v_features), therefore, we flatten the last two dimensions\n",
        "                into a single one (num_heads * v_features)\n",
        "            - final out is of shape  (batch_size, num_agents, num_heads * v_features)\n",
        "        \"\"\"\n",
        "\n",
        "        out = out.transpose(1,2).contiguous().view(batch_size, num_agents, -1)\n",
        "        out = self.activation(self.fc_out(out)) # Linear map into a desired feature dimension\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out, att_weights"
      ],
      "metadata": {
        "id": "Zol8QsIS9nS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <u>**Q-values Prediction**</u>\n",
        "We use **Q-Net** for Q-value predictions, a **reinforcement learning** technique that assigns values to each potential future action based on agent's observations (state). In this particular setting, the Q-Net predicts the expected reward for **each edge** the agent could take at any given step.\n",
        "\n",
        "To accomodate graphs with a varied number of edges per node, we implemented a **node mask** and generalized the setting to fit graphs with **variable** edge count for each node. The algorithm in Q-Net leverages dynamic programming weighted by the learning rate hyperparameter.\n",
        "\n",
        "$$Q_{target} = (1-l)*Q_{now}+l * E[R_{t+1}(a_{t+1}, s_{t+1}) + \\gamma * max_{a_{t+1}}Q_{next}(a_{t+1}, s_{t+1})| s_t],$$\n",
        "\n",
        "where $R_{t+1}(a_{t+1}, s_{t+1})$ is the reward received at timestep *t* after taking action $a_t$. These experiences are sampled from a batch of experiences and $l$ denotes learning rate. From this formulation, we can see that sampling the batch indices that maximize the temporal difference error, we essentially grow the $Q_{now}$ values for future steps.\n",
        "\n",
        "The goal of each agent is to maximize the expected future reward weighted by the gamma (discount) factor\n",
        "\n",
        "$$max_{(a)_{t_0}^T} E[\\sum_{t=t_0}^T \\gamma^{t-t_0}R_{t}(a_t, s_t)],$$\n",
        "where $\\gamma \\in (0,1)$."
      ],
      "metadata": {
        "id": "FN9jHalCI6UK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q-Net**\n",
        "In our **Q-learning** framework, the **Q-net** serves as the Q-function approximator, providing Q-values representing expected rewards.  While we closely follow the original implementation, we have introduced a slight but impactful adjustment: instead of utilizing a single linear layer to transform inputs into action values, we employ an **MLP**.\n",
        "\n",
        "This modification was necessary due to the **higher dimensionality of agent observations** resulting from the larger graph structure we are working with. By using an MLP, our architecture allows for a much smoother reduction that would occur if we had retained the original single linear layer structure. As a result, our Q-net can handle **more complex** and **high-dimensional data**.\n"
      ],
      "metadata": {
        "id": "65H4yFBgdJeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Q_Net(nn.Module):\n",
        "    \"\"\"\n",
        "    This servers as the Q-function  approximator in RL. It estimates Q-values for each possible action given a particular state.\n",
        "    What are Q-values? Rewards.\n",
        "    So, given a particular state, this estimates the expected future rewards(Q-values) for each possible action our plane(agent) can take.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, actions):\n",
        "        super(Q_Net, self).__init__()\n",
        "        self.fc = MLP(in_features, (2048,1024,512,actions), None, False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "CdjZwPGBeRTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DGN**\n",
        "**DGN (Deep Graph Neural Network)** integrates graph neural network with attention mechanisms to efficiently model interactions between agents. It starts with an encoder, that is represented by MLP, which processes the input features. The processed input is passed through a desired number of multi-head attention layers with masking to focus on relevant interactions. Finally, the output is fed into the Q-net, which estimates the Q-values.\n"
      ],
      "metadata": {
        "id": "_s_7VNFm91w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DGN(nn.Module):\n",
        "    \"\"\"\n",
        "        The Deep Graph Neural network. Incorporates attention mechanism.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, mlp_units, num_actions, num_heads, num_attention_layers, activation_fn, kv_values):\n",
        "        super(DGN, self).__init__()\n",
        "\n",
        "        self.encoder = MLP(in_features, mlp_units, activation_fn)\n",
        "        self.att_layers = nn.ModuleList()\n",
        "        hidden_features = self.encoder.out_features\n",
        "\n",
        "        print(\"In features of DGN: \", in_features)\n",
        "        print(\"MLP units are: \", mlp_units)\n",
        "\n",
        "        for _ in range(num_attention_layers):\n",
        "            self.att_layers.append(\n",
        "                AttModel(hidden_features, kv_values, kv_values, hidden_features, num_heads, activation_fn, activation_fn)\n",
        "                                   )\n",
        "\n",
        "        self.q_net = Q_Net(hidden_features * (num_attention_layers + 1), num_actions)\n",
        "\n",
        "        self.att_weights = []\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Additional comment to the function:\n",
        "            - each attention layer refines the representation h by focusing on relevant parts of the input\n",
        "            - by concatenating the representations the feature set for the Q-network is enhanced, consequently making more informed decisions\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        h = self.encoder(x)     # Encodes the input featuers, has a shape of (batch_size, num_agents, hidden_features)\n",
        "        q_input = h     # Initialize the q_input with encoded features\n",
        "        self.att_weights.clear()    # Ensuring that attention weights from previous forward passes do not accumulate\n",
        "\n",
        "        for attention_layer in self.att_layers:\n",
        "            h, att_weight = attention_layer(h, mask)\n",
        "            self.att_weights.append(att_weight)\n",
        "\n",
        "            # Concatenation of outputs\n",
        "            q_input = torch.cat((q_input, h), dim=-1)\n",
        "\n",
        "        # Final q_input is of shape (batch_size, num_agents, hidden_features * (num_attention_layers +1))\n",
        "        q = self.q_net(q_input)\n",
        "\n",
        "        return q    # is of shape (batch_size, num_agents, num_actions)\n"
      ],
      "metadata": {
        "id": "Pgg1EA3J94Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DQN**\n",
        "**DQN (Deep Q-Learning Network)** is a strandard architecture in Deep Q-Learning that leverages an MLP encoder to estimate **Q-values (expected rewards)**. The encoder transforms the incoming features into meaningful representations that are passed to the Q-network that estimates the Q-values.\n",
        "\n",
        "One might wonder, **why not use a single MLP for the entire process?** The reasoning is that separating the architectures allows for greater  versatility and scalability. This approach makes it easier to adapt and extend the model painlessly."
      ],
      "metadata": {
        "id": "G5-kiKLj98yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    Introduces simple Deep Feed Forward Neural Network( = MLP) as the encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, mlp_units, num_actions, activation_fn):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.encoder = MLP(in_features, mlp_units, activation_fn)   # Encodes incoming features\n",
        "        self.q_net = Q_Net(self.encoder.out_features, num_actions)  # Outputs Q-values\n",
        "        self.activation = activation_fn\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        batch, agent, features = x.shape\n",
        "        h = self.encoder(x)\n",
        "        q = self.q_net(h)\n",
        "        return q\n"
      ],
      "metadata": {
        "id": "Vx3Sl8CB99oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**CommNet**\n",
        "**CommNet (Communication Network)** is a specialized network designed to facilitate information exchange between multiple agents. CommNet builds upon the standard **DQNR (Deep Q-Network Architecture)** architecture, which utilized LSTMs forward passes, allowing for inter-agent communication. One can select a desirable amount of communication rounds between agents. Communication is restricted be the adjacency matrix that allows communication only between neighbours. Additionally, an LSTM forward pass is implemented to enhance cooperation. Overall, **CommNet is designed to foster collaboration and coordination strategies** in multi-agent environments for cooperative tasks.\n"
      ],
      "metadata": {
        "id": "VK8i3VvG-Bg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNR(nn.Module):\n",
        "    \"\"\"\n",
        "    Recurrent DQN with an lstm cell.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, mlp_units, num_actions, activation_fn):\n",
        "        super(DQNR, self).__init__()\n",
        "        self.encoder = MLP(in_features, mlp_units, activation_fn)\n",
        "        self.lstm = nn.LSTMCell(\n",
        "            input_size=self.encoder.out_features, hidden_size=self.encoder.out_features\n",
        "        )\n",
        "        self.state = None\n",
        "        self.q_net = Q_Net(self.encoder.out_features, num_actions)\n",
        "\n",
        "    def get_state_len(self):\n",
        "        return 2 * self.lstm.hidden_size\n",
        "\n",
        "    def _state_reshape_in(self, batch_size, n_agents):\n",
        "        \"\"\"\n",
        "        Reshapes the state of shape\n",
        "            (batch_size, n_agents, self.get_state_len())\n",
        "        to shape\n",
        "            (2, batch_size * n_agents, hidden_size).\n",
        "\n",
        "        :param batch_size: the batch size\n",
        "        :param n_agents: the number of agents\n",
        "        \"\"\"\n",
        "        self.state = (\n",
        "            self.state.reshape(\n",
        "                batch_size * n_agents,\n",
        "                2,\n",
        "                self.lstm.hidden_size,\n",
        "            )\n",
        "            .transpose(0, 1)\n",
        "            .contiguous()\n",
        "        )\n",
        "\n",
        "    def _state_reshape_out(self, batch_size, n_agents):\n",
        "        \"\"\"\n",
        "        Reshapes the state of shape\n",
        "            (2, batch_size * n_agents, hidden_size)\n",
        "        to shape\n",
        "            (batch_size, n_agents, self.get_state_len()).\n",
        "\n",
        "        :param batch_size: the batch size\n",
        "        :param n_agents: the number of agents\n",
        "        \"\"\"\n",
        "        self.state = self.state.transpose(0, 1).reshape(batch_size, n_agents, -1)\n",
        "\n",
        "    def _lstm_forward(self, x, reshape_state=True):\n",
        "        \"\"\"\n",
        "        A single lstm forward pass\n",
        "\n",
        "        :param x: Cell input\n",
        "        :param reshape_state: reshape the state to and from (batch_size, n_agents, -1)\n",
        "        \"\"\"\n",
        "        batch_size, n_agents, feature_dim = x.shape\n",
        "        # combine agent and batch dimension\n",
        "        x = x.view(batch_size * n_agents, -1)\n",
        "\n",
        "        if self.state is None:\n",
        "            lstm_hidden_state, lstm_cell_state = self.lstm(x)\n",
        "        else:\n",
        "            if reshape_state:\n",
        "                self._state_reshape_in(batch_size, n_agents)\n",
        "            lstm_hidden_state, lstm_cell_state = self.lstm(\n",
        "                x, (self.state[0], self.state[1])\n",
        "            )\n",
        "\n",
        "        self.state = torch.stack((lstm_hidden_state, lstm_cell_state))\n",
        "        x = lstm_hidden_state\n",
        "\n",
        "        # undo combine\n",
        "        x = x.view(batch_size, n_agents, -1)\n",
        "        if reshape_state:\n",
        "            self._state_reshape_out(batch_size, n_agents)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        h = self.encoder(x)\n",
        "        h = self._lstm_forward(h)\n",
        "        return self.q_net(h)\n",
        "\n",
        "\n",
        "class CommNet(DQNR):\n",
        "    \"\"\"\n",
        "        Communication Network employing inter-agent communication.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features,\n",
        "        mlp_units,\n",
        "        num_actions,\n",
        "        comm_rounds,\n",
        "        activation_fn,\n",
        "    ):\n",
        "        super().__init__(in_features, mlp_units, num_actions, activation_fn)\n",
        "        assert comm_rounds >= 0\n",
        "        self.comm_rounds = comm_rounds\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        batch_size, n_agents, feature_dim = x.shape\n",
        "        h = self.encoder(x)\n",
        "\n",
        "        # manually reshape state\n",
        "        if self.state is not None:\n",
        "            self._state_reshape_in(batch_size, n_agents)\n",
        "\n",
        "        h = self._lstm_forward(h, reshape_state=False)\n",
        "\n",
        "        # explicitly exclude self-communication from mask\n",
        "        mask = mask * ~torch.eye(n_agents, dtype=bool, device=x.device).unsqueeze(0)\n",
        "\n",
        "        for _ in range(self.comm_rounds):\n",
        "            # combine hidden state h according to mask\n",
        "            # first add up hidden states according to mask\n",
        "            #    h has dimensions (batch, agents, features)\n",
        "            #    and mask has dimensions (batch, agents, neighbors)\n",
        "            #    => we have to transpose the mask to aggregate over all neighbors\n",
        "            c = torch.bmm(h.transpose(1, 2), mask.transpose(1, 2)).transpose(1, 2)\n",
        "            # then normalize according to number of neighbors per agent\n",
        "            c = c / torch.clamp(mask.sum(dim=-1).unsqueeze(-1), min=1)\n",
        "\n",
        "            # skip connection for hidden state and communication\n",
        "            h = h + c\n",
        "            # use new hidden state\n",
        "            self.state[0] = h.view(batch_size * n_agents, -1)\n",
        "\n",
        "            # pass through forward module\n",
        "            h = self._lstm_forward(h, reshape_state=False)\n",
        "\n",
        "        # manually reshape state in the end\n",
        "        self._state_reshape_out(batch_size, n_agents)\n",
        "        return self.q_net(h)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IpEpQrpz-Ejb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <u>**Reward System: Detailed Description and Discussion**</u>\n",
        "Below we describe the reward system that we have created to promote efficient routing.\n",
        "\n",
        "#### **Reward Components**\n",
        "1. **Blocked Path Penalty:**\n",
        "   - *Condition:* If a plane attempts to traverse an edge whose load exceeds a threshold (1.5 = 3 planes), making it no-passable.\n",
        "   - *Penalty:* −10.0\n",
        "   - *Purpose:* The planes quickly learn that overloading an edge is harming. It is desirable that blocking happens during training as the planes are able to learn to avoid this scenario.\n",
        "   - Before the implementation of the prioritized based replay-buffer and with higher $\\gamma$ settings, planes were not able to experience being blocked as they explored too much and they got blocked very ofter during evaluation. We were able to tackle this issue effectively.\n",
        "\n",
        "2. **Shortest Path Incentive:**\n",
        "   - *Condition:* If the next node lies along the shortest path from the current node to the target node.\n",
        "   - *Reward:* +3.0\n",
        "   - *Purpose:* Encourages planes to follow the shortest paths for efficiency.\n",
        "\n",
        "3. **Progress-Based Incentive:**\n",
        "   - *Condition:* If the distance to the target decreases after taking an action.\n",
        "   - *Reward:* +1.5 * exp(0.002 * progress)\n",
        "   - *Purpose:* Rewards meaningful progress toward the target node and helps the plane to implicitly locate the target node.\n",
        "\n",
        "4. **Time Penalty:**\n",
        "   - *Condition:* Natural penalty incurred every action step.\n",
        "   - *Penalty:* −0.01\n",
        "   - *Purpose:* Encourages faster completion of the task by minimizing delays.\n",
        "\n",
        "5. **Near-Target Penalty:**\n",
        "   - *Condition:* If the plane is near the target (distance < 250) but chooses an action that doesn’t lead directly to the target in the next step.\n",
        "   - *Penalty:* −1.0\n",
        "   - *Purpose:* Reduces unnecessary detours near the destination and balances the reward for progress.\n",
        "\n",
        "6. **Looping Penalty:**\n",
        "   - *Condition:* If the plane revisits a node it has already visited.\n",
        "   - *Penalty:* −2.0\n",
        "   - *Purpose:* Discourages repeated visits to the same node.\n",
        "\n",
        "7. **Exploration Incentive:**\n",
        "   - *Condition:* If the plane visits a new node.\n",
        "   - *Reward:* +2.5\n",
        "   - *Purpose:* Encourages exploration of unvisited nodes.\n",
        "\n",
        "8. **Edge Traversal Adjustment:**\n",
        "   - *Condition:* While traversing an edge.\n",
        "     - *Shortest Path edge:* +0.2 per step.\n",
        "     - *Not Shortest Path edge:* −0.1 per step.\n",
        "   - *Purpose:* Maintains consistent evaluation during traversal.\n",
        "\n",
        "9. **Target Reached Reward:**\n",
        "   - *Condition:* Upon successfully reaching the target node.\n",
        "   - *Reward:* +10.0 * (1 + 1 / shortest path ratio)\n",
        "   - *Purpose:* Maximizes efficiency and incentivizes the shortest path.\n",
        "\n",
        "10. **Emergency Landing Penalty:**\n",
        "    - *Condition:* When no valid actions are possible (plane is \"dropped\").\n",
        "    - *Penalty:* −20.0\n",
        "    - *Purpose:* Penalizes failure to complete the task.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "This reward system strikes a balance between exploration, efficiency, and resource management. Positive rewards incentivize optimal pathfinding and exploration, while penalties discourage inefficiency, unnecessary detours, and overloading of edges. The design ensures that agents learn to navigate the network effectively while avoiding congested or suboptimal routes. Furthermore, after over 100 separate runs across a wide variety of settings, no loopholes have been identified in our reward system. Implementation of this reward can be found in the ```routing.py``` file within the **step** function."
      ],
      "metadata": {
        "id": "f_ZqrL8K8zqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <u>**State Aggregation**</u>\n",
        "\n",
        "### **SUM**\n",
        "**SUM** is a straighforward method of aggregating hidden node states during the message passing phase within the **NetMon** class.\n",
        "\n",
        "Node masking is implemented to ensure that only hidden states from neighbouring nodes is aggregated. This selective aggregation allows for creation of meaninigful representations for the underlying graph on which everything operates.\n",
        "\n",
        "Its primary advantage is **efficiency and speed** as was shown in the publishing paper [] and also within our work.\n"
      ],
      "metadata": {
        "id": "AGSMkNTI-KIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleAggregation(nn.Module):\n",
        "    def __init__(self, agg: str, mask_eye: bool) -> None:\n",
        "        super().__init__()\n",
        "        self.agg = agg\n",
        "        assert self.agg == \"mean\" or self.agg == \"sum\"\n",
        "        self.mask_eye = mask_eye\n",
        "\n",
        "    def forward(self, node_features, node_adjacency):\n",
        "        if self.mask_eye:\n",
        "            node_adjacency = node_adjacency * ~(\n",
        "                torch.eye(\n",
        "                    node_adjacency.shape[1],\n",
        "                    node_adjacency.shape[1],\n",
        "                    device=node_adjacency.device,\n",
        "                )\n",
        "                .repeat(node_adjacency.shape[0], 1, 1)\n",
        "                .bool()\n",
        "            )\n",
        "        feature_sum = torch.bmm(node_adjacency, node_features)\n",
        "        if self.agg == \"sum\":\n",
        "            return feature_sum\n",
        "        if self.agg == \"mean\":\n",
        "            num_neighbors = torch.clamp(node_adjacency.sum(dim=-1), min=1).unsqueeze(-1)\n",
        "            return feature_sum / num_neighbors\n"
      ],
      "metadata": {
        "id": "PCj3k7NC-NsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GCN**\n",
        "**GCN (Graph Convolutional Network)** is a graph\n",
        "\n",
        "GCN is a graph convolutional operator that during Message Passing phase within the GNN handles hidden state aggregation, much like the **SUM** method described above. Implementation is available at [GCN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv) within the Pytorch Geometric library that specializes on GNNs.\n",
        "\n",
        "We do not provide below any code for GCNConv class from Pytorch Geometric as we did not change it at all and do not want to implement it by ourselfs. At the very least, we provide a short description.\n",
        "It is a fundamental building block for GCNs that operate on graph-like/structured data. It performs convolution operations as a form of aggregation of hidden node states from neighbourhoods.\n",
        "\n"
      ],
      "metadata": {
        "id": "7S-b66SX-TyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <u>**NetMon**</u>\n",
        "**NetMon**, which is within ```model.py``` is the cornerstone of this system and it was meticulously designed by the original authors. This class is responsible for creating the underlying graph observations, which are then integrated into agent observations through the **NetMon environment wrapper** (within ```wrapper.py```). By utilizing a MP phase similar to those found in Graph Neural Networks (GNNs), **NetMon** recurrently updates and produces hidden states, which carry meaningful information about the underlying graph.\n",
        "\n",
        "### **State management**\n",
        "**LSTM**s **(Long Short-Term Memory)**/**GRU**s **(Gated Recurrent Unit)** are used to maintain and update the internal states of nodes over time. This approach effectively caputres **temporal dependencies** within the network, making this mechanism crucial for the routing task. Because of this system, agents can **adjust their paths dynamically** as the recurrent networks remember past patterns and are able to predict future ones."
      ],
      "metadata": {
        "id": "ojoJfBcX-Wrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NetMon(nn.Module):\n",
        "    \"\"\"\n",
        "    Why does this even exist?\n",
        "        - processing observations from nodes in the graph\n",
        "        - performs message passing\n",
        "        - aggregation of information from neighboring nodes\n",
        "        - updating node states with RNN\n",
        "        - produces features for nodes in the graph\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, hidden_features: int, encoder_units, iterations, activation_fn,\n",
        "                rnn_type=\"lstm\",\n",
        "                rnn_carryover=True, agg_type=\"sum\",\n",
        "                output_neighbor_hidden = False, output_global_hidden = False\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        assert isinstance(hidden_features, int)\n",
        "\n",
        "        # print(\"In-features to Netmon encoder:\", in_features)\n",
        "        self.encode = MLP(in_features, (*encoder_units, hidden_features), activation_fn)    # Define simple MLP as the endocer function\n",
        "        self.state = None\n",
        "        self.output_neighbor_hidden = output_neighbor_hidden\n",
        "        self.output_global_hidden = output_global_hidden\n",
        "        self.rnn_carryover = rnn_carryover\n",
        "        self.iterations = iterations\n",
        "\n",
        "        # 0 = dense input - dense matricies\n",
        "        # 1 = sparse input - sparse matricies\n",
        "        self.aggregation_def_type = None\n",
        "\n",
        "        # Agreggation\n",
        "        self.agg_type_str = agg_type    # GCN\n",
        "\n",
        "        # Now we will resolve the actual aggregation with the individual networks\n",
        "        if agg_type == \"sum\" or agg_type == \"mean\":\n",
        "            self.aggregate = SimpleAggregation(agg=agg_type, mask_eye=False)\n",
        "            self.aggregation_def_type = 0\n",
        "        elif agg_type == \"gcn\":\n",
        "            self.aggregate = GCNConv(hidden_features, hidden_features, improved=True)\n",
        "            self.aggregation_def_type = 1\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown aggregation type {agg_type}\")\n",
        "\n",
        "\n",
        "        # Update and observation encoding\n",
        "        self.rnn_type = rnn_type    # lstm in our\n",
        "        if self.rnn_type == \"lstm\":\n",
        "            self.rnn_obs = nn.LSTMCell(hidden_features, hidden_features)\n",
        "            self.rnn_update = nn.LSTMCell(hidden_features, hidden_features)\n",
        "            self.num_states = 2 if rnn_carryover else 4 # 2\n",
        "        elif self.rnn_type == \"gru\":\n",
        "            self.rnn_obs = nn.GRUCell(hidden_features, hidden_features)\n",
        "            self.rnn_update = nn.GRUCell(hidden_features, hidden_features)\n",
        "            self.num_states = 1 if rnn_carryover else 2\n",
        "        elif self.rnn_type == \"none\":\n",
        "            # empty state / stateless => simply store h for debugging\n",
        "            self.num_states = 1\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown rnn type {self.rnn_type}\")\n",
        "\n",
        "        self.hidden_features = hidden_features\n",
        "        self.state_size = hidden_features * self.num_states\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, mask, node_agent_matrix, max_degree=None, no_agent_mapping = False):\n",
        "        # This function contains steps (1), (2) and (3)\n",
        "        h, last_neighbor_h = self.update_node_states(x, mask)\n",
        "\n",
        "        # Step (4), Check what type of node states to aggregate. Either global or neighbor\n",
        "        if self.output_neighbor_hidden or self.output_global_hidden:\n",
        "            extended_h = [h]\n",
        "\n",
        "            # Aggregate neighbors\n",
        "            if self.output_neighbor_hidden:\n",
        "                extended_h.append(\n",
        "                        self.get_neighbor_h(last_neighbor_h, mask, max_degree)\n",
        "                    )\n",
        "\n",
        "            # Aggregate global\n",
        "            if self.output_global_hidden:\n",
        "                extended_h.append(self.get_global_h(h))\n",
        "\n",
        "            h = torch.cat(extended_h, dim=-1)   # Concatenates all features along the last dimension\n",
        "\n",
        "\n",
        "        if no_agent_mapping:\n",
        "            return h\n",
        "\n",
        "        return NetMon.output_to_network_obs(h, node_agent_matrix)\n",
        "\n",
        "\n",
        "    def get_state_size(self):\n",
        "        return self.state_size\n",
        "\n",
        "    def get_global_h(self, h):\n",
        "        \"\"\"\n",
        "            Computes a global summary of the nodes states and appends it to each Node's representation.\n",
        "        \"\"\"\n",
        "        _, n_nodes, _ = h.shape     # (batch_size, n_nodes, hidden_size)\n",
        "\n",
        "        \"\"\"\n",
        "            - mean(dim=1) computes the mean along all nodes for each batch\n",
        "                -> (batch_size, hidden_size)\n",
        "            - repeat repeats the global hidden state n_node times along a new dimension -> (n_nodes, batch_size, hidden_size)\n",
        "            - transpose resutls in (batch_size, n_nodes, hidden_size)\n",
        "        \"\"\"\n",
        "        global_h = h.mean(dim=1).repeat((n_nodes,1,1)).transpose(0,1)\n",
        "        return global_h\n",
        "\n",
        "    def get_neighbor_h(self, neighbor_h, mask, max_degree):\n",
        "        \"\"\"\n",
        "            Computes a summary based on Nodes neighbors and appends it to each Node's representation.\n",
        "        \"\"\"\n",
        "        batch_size, n_nodes , _ = neighbor_h.shape\n",
        "\n",
        "        # Get max node id for dense observation tensor (excludes self)\n",
        "        if max_degree is None:  # The maximum number of neigbors for each node -> if it is none -> compute from adjacency matrix\n",
        "            max_degree = torch.sum(mask, dim=1).max().long().item() - 1\n",
        "\n",
        "        # Pre-allocate a placeholder for each neighbor\n",
        "        h_neighbors = torch.zeros((batch_size, n_nodes, max_degree, neighbor_h.shape[-1]), device = neighbor_h.device)\n",
        "\n",
        "        # Get mask without self (pure neighbors)\n",
        "        neighbor_mask = mask * ~(                               # ~ is negation -> creates a matrix of ones where diagonal is 0\n",
        "            torch.eye(n_nodes, n_nodes, device=mask.device)\n",
        "            .unsqueeze(0)   # Add dimension to the 0th positions\n",
        "            .repeat(mask.shape[0], 1, 1) # Repeat the tensor mask.shape[0] times along the first dimension and once along the second and the third\n",
        "            .bool()\n",
        "        )\n",
        "\n",
        "        # Now we want to collect features from neighbors\n",
        "        h_index = neighbor_mask.nonzero()\n",
        "\n",
        "        # Get the relative neighbor ID for the insertion into h_neighbors\n",
        "        cumulative_neighbor_index = neighbor_mask.cumsum(dim=-1).long() - 1     # Cumulatively sums the neighbor mask along the last dimension to assign a unique index to each neighbor per node\n",
        "        h_neighbors_index = cumulative_neighbor_index[h_index[:,0], h_index[:, 1], h_index[:, 2]]\n",
        "\n",
        "        # Copy the last hidden state of all neighbors into the hidden state tensor\n",
        "        # For each neighbor connection, copies neighbor's hidden state into h_neighbors to corresponding position\n",
        "        h_neighbors[h_index[:,0], h_index[:, 1], h_neighbors_index] = neighbor_h[h_index[:,0], h_index[:, 2]]\n",
        "\n",
        "        # Concatenate info for each node\n",
        "        return h_neighbors.reshape(batch_size, n_nodes, -1)     # Reshape from (batch_size, n_nodes, max_degree, hidden_size) to (batch_size, n_nodes, max_degree*hidden_size)\n",
        "                                                                # |\n",
        "                                                                #  -> each node has a concatenated vector of its neighbors' hidden states\n",
        "\n",
        "\n",
        "    def update_node_states(self, x, mask):\n",
        "        \"\"\"\n",
        "            This function performs message passing and state updates over a specified number of iterations.\n",
        "            It integrates both node features and graph structure.\n",
        "\n",
        "            :mask: it is the adjacency matrix of the graph -> (n_waypoints,n_waypoints)\n",
        "        \"\"\"\n",
        "        batch_size, n_nodes, feature_dim = x.shape # (1, 131, 1463)\n",
        "\n",
        "        x = x.reshape(batch_size * n_nodes, -1)  # New shape is (batch_size * n_nodes, feature_dim)\n",
        "\n",
        "        if self.state == None: # For storing hidden states\n",
        "            # Init\n",
        "            self.state = torch.zeros((batch_size, n_nodes, self.state_size), device = x.device)\n",
        "\n",
        "        # Reshape the state before further processing\n",
        "        self.state_reshape_in(batch_size, n_nodes)\n",
        "\n",
        "        # step (1): encode observation to get h^0_v and combine with state\n",
        "        h = self.encode(x)  # Producing initial hidden representations\n",
        "\n",
        "        # Choose what we are using. Either LSTM or GRU\n",
        "        if self.rnn_type in [\"lstm\", \"lnlstm\"]:\n",
        "            h0, cx0 = self.rnn_obs(h, (self.state[0], self.state[1])) # rnn.obs processes the encoded features h along with the previous states\n",
        "            h, cx = h0, cx0\n",
        "\n",
        "        # Message passing iterations\n",
        "        if self.iterations <= 0 and self.output_neighbor_hidden:\n",
        "            last_neighbor_h = torch.zeros_like(h, device=h.device)  # Returns a tensor filled with 0s in the shape of h\n",
        "        else:\n",
        "            last_neighbor_h = None\n",
        "\n",
        "        if self.aggregation_def_type != 0:\n",
        "            mask_sparse, mask_weights = dense_to_sparse(mask)   # Conversion to a sparse representation for the aggreagtion function\n",
        "\n",
        "        if self.aggregation_def_type == 2:\n",
        "            H, C = self.state[0], self.state[1] # Init of additional aggregation types\n",
        "\n",
        "        # Iteration\n",
        "        for it in range(self.iterations):\n",
        "            if self.output_neighbor_hidden and it == self.iterations-1:\n",
        "                if self.aggregation_def_type == 2:\n",
        "                    \"\"\"\n",
        "                        we know that the aggregation step will exchange the hidden states\n",
        "                        (and much more..) so we can just use them for the skip connection\n",
        "                        instead of the other nodes' input.\n",
        "                        This is only relevant for a single iteration per step.\n",
        "                    \"\"\"\n",
        "                    last_neighbor_h = H\n",
        "                else:\n",
        "                    # use the last received hidden state\n",
        "                    last_neighbor_h = h\n",
        "\n",
        "            # step (2): aggregate - computes the aggregated messages M for each node.\n",
        "            if self.aggregation_def_type == 0:  # Simple aggregation\n",
        "                M = self.aggregate(h.view(batch_size, n_nodes, -1), mask).view(\n",
        "                    batch_size * n_nodes, -1\n",
        "                )\n",
        "            elif self.aggregation_def_type == 1:    # Aggregation through conv. layers with sparse mask\n",
        "                M = self.aggregate(h, mask_sparse)\n",
        "\n",
        "            elif self.aggregation_def_type == 2:    # Specialized aggregation with additional states\n",
        "                H, C = self.aggregate(h, mask_sparse, H=H, C=C)\n",
        "                M = H\n",
        "            elif self.aggregation_def_type == 3:    # Uses models like GraphSAGE etc. proly won't be useful to us\n",
        "                # overwrite last_neighbor_h with jumping knowledge output\n",
        "                M, last_neighbor_h = self.aggregate(h, mask_sparse)\n",
        "\n",
        "            # step (3): update - it is performed using RNN cell with the aggregated messages M\n",
        "            \"\"\"\n",
        "                What is the carryover mechanism?\n",
        "                    Carryover mechanism controls whether to carry over states between iterations or reset them.\n",
        "            \"\"\"\n",
        "            if self.rnn_type in [\"lstm\", \"lnlstm\"]:\n",
        "                if not self.rnn_carryover and it == 0:\n",
        "                    rnn_input = (self.state[2], self.state[3])\n",
        "                else:\n",
        "                    rnn_input = (h, cx)\n",
        "\n",
        "                h1, cx1 = self.rnn_update(M, rnn_input)\n",
        "                h, cx = h1, cx1\n",
        "            elif self.rnn_type == \"gru\":\n",
        "                if not self.rnn_carryover and it == 0:\n",
        "                    rnn_input = self.state[1]\n",
        "                else:\n",
        "                    rnn_input = h\n",
        "                h1 = self.rnn_update(M, rnn_input)\n",
        "                h = h1\n",
        "            else:\n",
        "                h = M\n",
        "\n",
        "        # Reshaping\n",
        "        if last_neighbor_h is not None:\n",
        "            last_neighbor_h = last_neighbor_h.reshape(batch_size, n_nodes, -1)  # Reshaping to original dimensions for output\n",
        "        h = h.reshape(batch_size, n_nodes, -1)  # Reshaping to original dimensions for output\n",
        "\n",
        "        # Updating of the internal state\n",
        "        if self.rnn_type in [\"lstm\", \"lnlstm\"]:\n",
        "            if self.rnn_carryover:\n",
        "                self.state = torch.stack((h1, cx1))     # Concatenating tensors along a new dimension\n",
        "            else:\n",
        "                self.state = torch.stack((h0, cx0, h1, cx1))\n",
        "        elif self.rnn_type == \"gru\":\n",
        "            if self.rnn_carryover:\n",
        "                self.state = h1.unsqueeze(0)\n",
        "            else:\n",
        "                self.state = torch.stack((h0.unsqueeze(0), h1.unsqueeze(0)))\n",
        "        elif self.rnn_type == \"none\":\n",
        "            # store last node state for debugging and aux loss\n",
        "            self.state = h.unsqueeze(0)\n",
        "\n",
        "        self.state_reshape_out(batch_size, n_nodes)\n",
        "\n",
        "        return h, last_neighbor_h\n",
        "\n",
        "    def state_reshape_in(self, batch_size, n_agents):\n",
        "        \"\"\"\n",
        "            Reshapes the state of shape (batch_size, n_agents, self.get_state_len())\n",
        "                to shape\n",
        "                    (2, batch_size * n_agents, hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        if self.state.numel() == 0:\n",
        "            return\n",
        "\n",
        "        self.state = self.state.reshape(batch_size * n_agents, self.num_states, -1).transpose(0,1)\n",
        "\n",
        "    def state_reshape_out(self, batch_size, n_agents):\n",
        "        \"\"\"\n",
        "            Reshapes the state of shape\n",
        "                (2, batch_size * n_agents, hidden_size)\n",
        "            to shape\n",
        "                (batch_size, n_agents, self.get_state_len()).\n",
        "\n",
        "            :param batch_size: the batch size\n",
        "            :param n_agents: the number of agents\n",
        "        \"\"\"\n",
        "        if self.state.numel() == 0:\n",
        "            return\n",
        "\n",
        "        self.state = self.state.transpose(0, 1).reshape(batch_size, n_agents, -1)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def output_to_network_obs(netmon_out, node_agent_matrix):\n",
        "        \"\"\"\n",
        "            Netmon_out is called within the forward function. Why? It performs the mapping of the node information to agents.\n",
        "                It multiplies the node outputs with node_agent_matrix to aggregate/map node outputs to agent-specific outputs.\n",
        "        \"\"\"\n",
        "\n",
        "        # bmm performs a batch matrix-matrix product of matricies stored in netmon_out.transpose(1,2) and node_agent_matrix\n",
        "        return torch.bmm(netmon_out.transpose(1, 2), node_agent_matrix).transpose(1, 2)"
      ],
      "metadata": {
        "id": "bHUwK40rJ9SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <u>**Multi GPU setups**</u>\n",
        "##**Bi-GPU setup**\n",
        "In **our bi-GPU configuration**, we have adapted the primary script (```main.py``` to ```bi_gpu_main.py```) and the environment wrapper to utilize two GPUs. A key modification involves assigning the Q-value predictor to a **separate** GPU. This separation allows us to handle larger mini-batch sizes by distributing the computational load more evenly across both GPUs. By adjusting the device allocation within ```bi_gpu_main.py```, we ensuree that different components of the model are optimally assigned to each GPU.\n",
        "\n",
        "Additionally, the environment wrapper has been updated to manage data transfer between the two GPUs. This includes managing inter-GPU communication to maintain consistency during the training process. Specifically, dedicating the Q-value predictor to separate GPU reduces the memory and processing strain on the primary GPU, thereby enabling higher throughput and faster training times.\n",
        "\n",
        "##**Multi-GPU setup**\n",
        "To scale our setup to multiple GPUs, we incorporated PyTorch's **DataParallel** class, which allows the model to be distributed across several GPUs within a single compute node. This approach offers the flexibility to handle varying numbers of GPUs without requiring significant alterations. By leveraging **DataParallel**, we split the model across available GPUs, enabling parallel processing of input batches $→$ increasing computational efficiency.\n",
        "\n",
        "A crucial modification was made to the NetMon class. Originally, **NetMon** maintained its internal state as an attribute, which posed challenges for parallel execution since DataParallel class replicates the model across available GPUs. To overcome this, we r**emoved** the internal state attribute from NetMon, ensuring that state information is managed externally rather than being tied to a specific instance of the model. Furthermore, the entire pipeline, including the **NetMon environment wrapper**, was redesigned to return the internal state during the forward pass. This adjustment ensures compatibility with DataParallel, which requires modules to be **stateless**.\n"
      ],
      "metadata": {
        "id": "Pw68VqsjYacn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <u>**Selecting Parameters**</u>\n",
        "To optimize othe performance of our models, we conducted multiple hyperparameter sweeps using Weights & Biases (W&B). These sweeps provided valuable insights into the importance and correlations of various parameters. Although project time constraints limited the extent of our data collection, the results obtained are sufficiently clear to guide our parameter selection. We selected the best-performing models based on the accumulated rewards. Additionally, we considered other significant metrics such as SPR (the mean ratio of the path length taken to the true shortest path) and throughput (the mean ratio of planes that reached their targets during an episode).\n",
        "\n",
        "We ran two sweeps, one for only CommNet settings (first picture), the other sweep for comparing DQN and DGN (second picture). Both sweeps ran for 75k steps, with CommNet being significantly faster yet inferior architecture and Sum being the superior AND faster aggregation method.\n",
        "\n",
        "Both sweeps were conducted over 75k steps. Our findings indicate that while **CommNet** (first figure) is significantly faster, it demonstrated inferior performance compared to other architectures (second picture). Conversely, the SUM aggregation method proved to be both **superior in performance and faster in execution**.\n",
        "\n",
        "##**Common Parameters in the Sweep**\n",
        " - **mini_batch_size** : number of sampled experiences from the replay buffer\n",
        " - **epsilon_decay** : decay factor of epsilon in the EpsilonGreedy policy\n",
        " - **agg_type** : method used during NetMon message passing phase $→$ produces node representations\n",
        " - **gamma**: discount factor in Q-Learning\n",
        "\n",
        "## **CommNet specific**\n",
        " - **comm_round**: number of information passing round\n",
        "\n",
        "## **DQN, DGN specific**\n",
        " - **num_heads**: number of attention heads\n",
        " - **att_layers**: number of attention layers\n",
        "\n"
      ],
      "metadata": {
        "id": "1Ei12632gg3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# <u>**Advice for Parameter Selection**</u>\n",
        "In this section, we provide guidance on fine-tuning hyperparameters, followed by results from 35 sweeps conducted on Weights & Biases (W&B) across two different settings.\n",
        "## **CommNet settings**\n",
        "Our experiments reveal that using a **lower number of communication rounds** (com_rounds) and **increasing the mini-batch size** significantly enhances the model's performance. Although the epsilon update frequency did not appear as critical, this may be due to the limited range we explored during the sweep. We recommend setting this parameter to around **70 for 75k steps** and gradually increasing it as the number of training steps grows. A good rule of thumb is to ensure that after training, the epsilon value stabilizes around **0.01**. If epsilon falls below 0.01, it is reset to maintain some exploration.\n",
        "## **DQN, DGN settings**\n",
        "In the **DQN** and **DGN** settings, the discount factor (gamma) emerged as **the most crucial** hyperparameter. This finding aligns with expectations, given that the architecture directly feeds into the **Q-Net**. We observed consistent improvements in accumulated rewards when gamma was set lower (approximately **0.92-0.95**) over **75k steps**.\n",
        "\n",
        "Additionally, the epsilon update frequency proved to be highly significant, showing a strong negative correlation with performance. This indicates that epsilon should be updated more frequently, enabling the models to adapt and navigate the environment more efficiently. Furthermore, the **number of attention heads** and **attention layers** demonstrated a positive correlation with rewards, suggesting that increasing these parameters can further enhance performance.\n",
        "\n",
        "## **Aggregation Type**\n",
        "Across both sweeps, the **choice of aggregation type** did not show substantial importance. However, for **CommNet**, we recommend using **GCN**, as it exhibited a positive correlation with the generated rewards, whereas **SUM** showed a negative correlation.\n",
        "\n",
        "On the other hand, in the case of **DQN** and **DGN**, **SUM** performed better. Based on our extensive training experience, we recommend using SUM for its speed and comparable results."
      ],
      "metadata": {
        "id": "OhkNKLaYlGRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **WANDB sweep for CommNet**\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/commnet.png?raw=true)"
      ],
      "metadata": {
        "id": "e1gjgCZZzhzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hyperparameters importance for CommNet**\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/commnet_parameters.png?raw=true)"
      ],
      "metadata": {
        "id": "ghDCmQ55xYYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **WANDB sweep for DQN vs. DGN**\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/dgn_dqn.png?raw=true)"
      ],
      "metadata": {
        "id": "TO3iriopz9P7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **HYPERPARAMETERS IMPORTANCE FOR DGN and DQN**\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/dgn_dqn_parameters.png?raw=true)"
      ],
      "metadata": {
        "id": "ca4DwESCxrjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <u>**Our Results**</u>\n",
        "\n",
        "We trained multiple models with various settings but after performing the sweeps on Weights and Biases, our mean reward nearly doubled and so did the throughput of planes into targets. What's more interesting is that we observed a clean positive correlation between reward and throughput, meaning that with higher mean reward achieved the number of planes that landed was also higher. This effectively means that the agents did not find a loophole in the reward system, which is very pleasant and we are very intrigued by this result. These claims deserve more backing and rigorous testing as well as the before mentioned sweeps of the hyperparameter space. Unfortunatelly, we didn't have enough time during this project to give attention to all these things that arguably deserve it.\n",
        "\n",
        "\n",
        "Through extensive training with various model settings and conducting hyperparameter sweeps on Weights & Biases (W&B), we achieved remarkable improvements in our system's performance. Specifically, our **mean reward nearly doubled**, and the **throughput of planes reaching their targets also doubled**.\n",
        "\n",
        "\n",
        "## <u>**Our Selected Best Performing Models**</u>\n",
        "We present the top two models we have selected based on mean reward. Note that higher mean reward very likely implies higher throuput and smaller shortest path ratio. We also added a third model for comparison to include every architecture we used.\n",
        "\n",
        "### **Key observations**\n",
        "- We observed a **clear positive** correlation between the mean reward and the throughput of planes. This means that as the agents achieved higher mean rewards, the number of planes successfully landing at their targets also increased.\n",
        "- The positive correlation **reassures** us that the agents **did not** find unintended shortcuts or loopholes to artificially inflate rewards. Instead, they genuinely improved their performance by making more effective decisions. This outcome is of **extremely high importance** to us, as it validates the robustness of our reward design!! and the overall reliability of our training process. (We had an extremely hard time of deriving a working reward system that would promote effective routing in a large graph.)\n",
        "\n",
        "\n",
        "### **Hyperparameter search**\n",
        "Due to the limited time, we were not able to perform a fully rigorous hyperparameter search. Nevertheless, the current results provide a **strong foundation for future work**.\n",
        "\n"
      ],
      "metadata": {
        "id": "S8nU4WQLwJI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<style>\n",
        "    table {\n",
        "        width: 200%;\n",
        "    }\n",
        "</style>\n",
        "| Parameters               | Model 1          | Model 2          | Model 3        |\n",
        "|-------------------------|------------------------|------------------------|------------------------|\n",
        "| model_type              | comm_net               | dqn                    | dgn                    |\n",
        "| iterations              | 6                      | 6                      | 8                      |\n",
        "| agg_type                | gcn                    | gcn                    | sum                    |\n",
        "| att_layers              | -                     | -                      | 6                      |\n",
        "| num_heads            | - |        -             | 12|\n",
        "| kv_values               | -                     | -                     | 16                     |\n",
        "| comm_rounds             | 4                      | -                      | -                      |\n",
        "| epsilon_update_freq     | 70                     | 90                     | 70                     |\n",
        "| total_steps             | 75000                  | 75000                  | 75000                  |\n",
        "| step_before_train       | 15000                  | 15000                  | 15000                  |\n",
        "| step_between_train      | 10                     | 10                     | 5                      |\n",
        "| sequence_length         | 16                     | 16                     | 16                     |\n",
        "| gamma                   | 0.93                   | 0.91                   | 0.91                   |\n",
        "| mini_batch_size         | 32                     | 64                     | 32                     |\n",
        "|-------------------------|------------------------|------------------------|------------------------|\n",
        "|Results|\n",
        "|-------------------------|------------------------|------------------------|------------------------|\n",
        "| delays_mean             | 20.7              | **17.4**              | 28.6              |\n",
        "| delays_arrived_mean     | 21.8             | **18**              | 29              |\n",
        "| spr_mean                | 3.7               | **3**               | 5              |\n",
        "| looped_mean             | 0.41               | **0.05**               | 1.3               |\n",
        "| throughput_mean         | 0.42                  | **0.51**               | 0.29               |\n",
        "| dropped_mean            | 0.0                    | 0.0                    | 0.0                    |\n",
        "| blocked_mean            | 0.001               | 0.002               | 0.001               |\n",
        "| total_edge_load_mean    | 5               | 5               | 5               |\n",
        "| occupied_edges_mean     | 9.4              | 9.4               | 9.3               |\n",
        "| planes_on_edges_mean    | 9.99               | 9.99               | 9.99               |\n",
        "| total_plane_size_mean   | 5.0                    | 5.0                    | 5.0                    |\n",
        "| plane_sizes_mean        | 0.5                    | 0.5                    | 0.5                    |\n",
        "| plane_distances_mean    | 9.3                | 9.6               | 9.1               |\n",
        "| reward_mean             | 3.5              | **3.9**               | 2.7               |\n"
      ],
      "metadata": {
        "id": "eNkykOCkxXlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <u>**Heatmaps**</u>\n",
        "Heatmaps show the edge and node usage by the agents. Additional pictures can be found in the ```/pictures/evaluation_pictures``` directory or can be generated by the provided code. After each evaluation phase, the code automatically saves many interesting and informative pictures. These were taken from the original implementation and slightly <u>**beautified**</u>."
      ],
      "metadata": {
        "id": "K0VfxzH_jn4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **COMMNET**\n",
        "Utilization is concentraded aournd the central nodes with a balanced distribution across edges and the edge utilization appears to be more evenly distributed compared to the other models.\n",
        "\n",
        "\\\n",
        "\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/evaluation_pictures/heatmapcommnetmask.png?raw=true)"
      ],
      "metadata": {
        "id": "8GQ6b3ibeK6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DQN**\n",
        "Node and edge utilization still centers around the same critical nodes but shows a more pronounced preference for specific paths.\n",
        "\n",
        "\\\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/evaluation_pictures/heatmapdqnmask.png?raw=true)"
      ],
      "metadata": {
        "id": "eNq4qi-Pjbwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DGN**\n",
        "Higher utilization is concentrated around fewer nodes, indicating potentially fewer but more optimized routes. Additionally, some edges show much higher utilization than others, hinting at more focused path selection compared to DQN or COMMNET.\n",
        "\n",
        "\\\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/evaluation_pictures/heatmapdgnmask.png?raw=true)"
      ],
      "metadata": {
        "id": "xkXvwNuhkwSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<u>**Shortest Path Ratio**</u>\n",
        "These plots show the relationship between the number of steps in the shortest path to the target and the number of steps the agent took to reach it. The linear line in the middle represents the \"lower bound,\" but the number of steps must be scaled by the planes' speed for this interpretation to be strictly valid. Here, it simply serves as a reference for comparing the stability of the models.\n",
        "\n",
        "We can make an interesting observation that with the growing number of steps needed to reach the target, the number of steps the agents take to reach it grows almost linearly with some deviations. The **biggest instability** is in the DGN model, while DQN appears to be **exceptionally stable**. In the case of CommNet, the model behaves **even better** for longer paths, whis is simply lovely."
      ],
      "metadata": {
        "id": "MMiw4WCyk_W9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **COMMNET**\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/evaluation_pictures/sprcommnetmask.png?raw=true)"
      ],
      "metadata": {
        "id": "-qaQ7fj_mNZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DQN**\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/evaluation_pictures/sprdqnmask.png?raw=true)"
      ],
      "metadata": {
        "id": "E3XestfLmXa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DGN**\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/evaluation_pictures/sprdgnmask.png?raw=true)"
      ],
      "metadata": {
        "id": "K-eaJH9tmbec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <u>**Conclusion**</u>\n",
        "In this project, we explored the integration of Deep Graph Neural Networks (DGN) with reinforcement learning techniques to enhance multi-agent coordination and decision-making. We have successfully adapted previous work for our task of efficent air traffic control - efficient routing for planes in the airspace.\n",
        "\n",
        "Our investigation into different architectures, including CommNet, DQN, and DGN, revealed critical insights into the importance of communication rounds, mini-batch sizes, discount factors, and aggregation methods. The implementation of a prioritized replay buffer further augmented our model's ability to learn from impactful experiences, enhancing its capability to predict and navigate optimal paths effectively. Additionally, node masking allowed us to generalize this solution of graphs with diverse neighbourhoods.\n",
        "\n",
        "Future work could involve more exhaustive hyperparameter sweeps, longer training durations. Moreover, the system could be replicated across multiple regions and creating controller-subcontroller system, where each sub-controller specializes in managing a specific area.\n"
      ],
      "metadata": {
        "id": "buWgEb9M5rJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special Thanks**\n",
        "\n",
        "We would like to extend our gratitude to the FNSPE faculty for providing the computational resources necessary for this project, including access to the HELIOS cluster. Additionally, we would like to thank Tomáš Kerepecký for his assistance in setting up WANDB on the UTIA computing cluster."
      ],
      "metadata": {
        "id": "UpWrxcas7sAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other considered approaches**\n",
        "\n",
        "\n",
        "These methods were considered, but not selected either for their complexity or applicability.\n",
        "\n",
        "- [MALR on coordination graphs](https://medium.com/@jamgochian95/multi-agent-reinforcement-learning-with-coordination-graphs-428dddb99907)\n",
        "- [Former based approach for shortest path](https://medium.com/octavian-ai/finding-shortest-paths-with-graph-networks-807c5bbfc9c8)\n",
        "- [GNN for shortest path](https://medium.com/@bnn_upc/computing-the-shortest-path-with-graph-neural-networks-gnn-a-hands-on-introduction-to-ignnition-bea531b3b5b2)\n",
        "- [Interesting non-linear approach](https://www.sciencedirect.com/science/article/pii/S0096300306016304)\n",
        "- [Approximation of the shortest path](https://arxiv.org/pdf/2002.05257)\n",
        "- [Meta Learning intro](https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a)\n",
        "- [Multi agent meta learning](https://signalprocessingsociety.org/publications-resources/ieee-open-journal-signal-processing/dif-maml-decentralized-multi-agent-meta)\n",
        "- [Deep path](https://sites.cs.ucsb.edu/~william/papers/DeepPath.pdf)\n",
        "- [Shortest path with attention network](https://www.ijcai.org/proceedings/2019/569)\n",
        "\n",
        "**Honorable mentions**\n",
        "- [NN with Particle Swarm Optimization (PSO)](https://ojs.aaai.org/index.php/SOCS/article/view/18244)\n",
        "- [Growing Neural Gas](https://en.wikipedia.org/wiki/Neural_gas)\n"
      ],
      "metadata": {
        "id": "bj4yoGxfBeCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "- Graph MARL GitHub Repository: [Original Implementation][1]\n",
        "- Replay Buffer Implementation: [Replay Buffer GitHub][2]\n",
        "- [PyTorch Documentation][3]\n",
        "- [PyTorch Geometric Documentation][4]\n",
        "\n",
        "[1]: https://github.com/jw3il/graph-marl/tree/main?tab=readme-ov-file\n",
        "[2]: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/rl/dqn/replay_buffer.py\n",
        "[3]: https://pytorch.org/\n",
        "[4]: https://pytorch-geometric.readthedocs.io/en/latest/\n"
      ],
      "metadata": {
        "id": "XI6rlOjqEeNN"
      }
    }
  ]
}