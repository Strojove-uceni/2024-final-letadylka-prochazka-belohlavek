{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo file\n",
        "Authors: Michal Belohlavek, Tomas Prochazka\n",
        "## Intro\n",
        "Welcome to the demo file, where one can run the project with zero effort and see the results and visualistaion for themselves. While this is an easy and plesant way to enjoy this Neural Net, we strongly urge everyone to run the project as it was intended, expand upon it and improve it. If you seek a commented version of the code in the most miniscule detail, please take a look at our implementation in the GitHub repo.\n",
        "## Abstract\n",
        "This project was created and submited as the final semestral project for the Machine Learning 2 class on FNSPE CTU. We take the structure and code from the authors of Multi-Agent Reinforcement Learning in Graphs (reference in README). We modified, reshaped and added valuable parts to the original implementation to make it applicable to the problem of free routing and plane path navigation. The core aim of this project is to provide a rather fast neural network that navigates multiple planes along a graph with dynamically changing weights with the goal of reaching the target as fast as possible, while avoiding collisions. In this demo file, we demonstrate our work, provide an overview of the used techniques and give advice on how to select fine tuned hyperparameters.\n",
        "\n",
        "*For additional details, see our presentation on GitHub.*\n",
        "## Shoert overview of the used ML techniques\n",
        "\n",
        "## How to run the project\n",
        "### Selecting parameters\n",
        "### Advice for parameter selection\n",
        "\n",
        "## Our results\n"
      ],
      "metadata": {
        "id": "Mey2sVO3w06m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/\n",
        "%cd /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Qg4P8isqT8SB",
        "outputId": "743336e1-d1d3-48a0-cdad-9eedc8ea80b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '2024-final-letadylka-prochazka-belohlavek' already exists and is not an empty directory.\n",
            "[Errno 20] Not a directory: '/content/2024-final-letadylka-prochazka-belohlavek/requirements.txt'\n",
            "/content\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the main.py script with the modified config\n",
        "!python /content/2024-final-letadylka-prochazka-belohlavek/src/main.py --config data/config_comm.yaml"
      ],
      "metadata": {
        "id": "ypBUnsbfUgem",
        "outputId": "ceaacb0c-9e07-4bca-8475-f6aeb1b682e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-12 10:11:59.841513: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-12 10:11:59.878132: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-12 10:11:59.887487: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-12 10:11:59.924750: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.0 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/content/2024-final-letadylka-prochazka-belohlavek/src/main.py\", line 29, in <module>\n",
            "    from torch.utils.tensorboard.writer import SummaryWriter\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py\", line 12, in <module>\n",
            "    from .writer import FileWriter, SummaryWriter  # noqa: F401\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\", line 19, in <module>\n",
            "    from ._embedding import get_embedding_info, make_mat, make_sprite, make_tsv, write_pbtxt\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/_embedding.py\", line 10, in <module>\n",
            "    _HAS_GFILE_JOIN = hasattr(tf.io.gfile, \"join\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 65, in __getattr__\n",
            "    return getattr(load_once(self), attr_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 97, in wrapper\n",
            "    cache[arg] = f(arg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 50, in load_once\n",
            "    module = load_fn()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py\", line 45, in tf\n",
            "    import tensorflow\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\", line 47, in <module>\n",
            "    from tensorflow._api.v2 import __internal__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 8, in <module>\n",
            "    from tensorflow._api.v2.__internal__ import autograph\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py\", line 8, in <module>\n",
            "    from tensorflow.python.autograph.core.ag_ctx import control_status_ctx # line: 34\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/core/ag_ctx.py\", line 21, in <module>\n",
            "    from tensorflow.python.autograph.utils import ag_logging\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/utils/__init__.py\", line 17, in <module>\n",
            "    from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/utils/context_managers.py\", line 19, in <module>\n",
            "    from tensorflow.python.framework import ops\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 46, in <module>\n",
            "    from tensorflow.python import pywrap_tfe\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/pywrap_tfe.py\", line 25, in <module>\n",
            "    from tensorflow.python._pywrap_tfe import *\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py\", line 42, in tf\n",
            "    from tensorboard.compat import notf  # noqa: F401\n",
            "ImportError: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "AttributeError: _ARRAY_API not found\n",
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.0 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/content/2024-final-letadylka-prochazka-belohlavek/src/main.py\", line 29, in <module>\n",
            "    from torch.utils.tensorboard.writer import SummaryWriter\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py\", line 12, in <module>\n",
            "    from .writer import FileWriter, SummaryWriter  # noqa: F401\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\", line 19, in <module>\n",
            "    from ._embedding import get_embedding_info, make_mat, make_sprite, make_tsv, write_pbtxt\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/_embedding.py\", line 10, in <module>\n",
            "    _HAS_GFILE_JOIN = hasattr(tf.io.gfile, \"join\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 65, in __getattr__\n",
            "    return getattr(load_once(self), attr_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 97, in wrapper\n",
            "    cache[arg] = f(arg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 50, in load_once\n",
            "    module = load_fn()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py\", line 45, in tf\n",
            "    import tensorflow\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\", line 47, in <module>\n",
            "    from tensorflow._api.v2 import __internal__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 8, in <module>\n",
            "    from tensorflow._api.v2.__internal__ import autograph\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py\", line 8, in <module>\n",
            "    from tensorflow.python.autograph.core.ag_ctx import control_status_ctx # line: 34\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/core/ag_ctx.py\", line 21, in <module>\n",
            "    from tensorflow.python.autograph.utils import ag_logging\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/utils/__init__.py\", line 17, in <module>\n",
            "    from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/utils/context_managers.py\", line 19, in <module>\n",
            "    from tensorflow.python.framework import ops\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 49, in <module>\n",
            "    from tensorflow.python.client import pywrap_tf_session\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/pywrap_tf_session.py\", line 19, in <module>\n",
            "    from tensorflow.python.client._pywrap_tf_session import *\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py\", line 42, in tf\n",
            "    from tensorboard.compat import notf  # noqa: F401\n",
            "ImportError: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "AttributeError: _ARRAY_API not found\n",
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.0 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/content/2024-final-letadylka-prochazka-belohlavek/src/main.py\", line 29, in <module>\n",
            "    from torch.utils.tensorboard.writer import SummaryWriter\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py\", line 12, in <module>\n",
            "    from .writer import FileWriter, SummaryWriter  # noqa: F401\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\", line 19, in <module>\n",
            "    from ._embedding import get_embedding_info, make_mat, make_sprite, make_tsv, write_pbtxt\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/_embedding.py\", line 10, in <module>\n",
            "    _HAS_GFILE_JOIN = hasattr(tf.io.gfile, \"join\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 65, in __getattr__\n",
            "    return getattr(load_once(self), attr_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 97, in wrapper\n",
            "    cache[arg] = f(arg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 50, in load_once\n",
            "    module = load_fn()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py\", line 45, in tf\n",
            "    import tensorflow\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\", line 47, in <module>\n",
            "    from tensorflow._api.v2 import __internal__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 11, in <module>\n",
            "    from tensorflow._api.v2.__internal__ import distribute\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py\", line 8, in <module>\n",
            "    from tensorflow._api.v2.__internal__.distribute import combinations\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py\", line 8, in <module>\n",
            "    from tensorflow.python.distribute.combinations import env # line: 456\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/combinations.py\", line 33, in <module>\n",
            "    from tensorflow.python.distribute import collective_all_reduce_strategy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 25, in <module>\n",
            "    from tensorflow.python.distribute import cross_device_ops as cross_device_ops_lib\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/cross_device_ops.py\", line 28, in <module>\n",
            "    from tensorflow.python.distribute import cross_device_utils\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/cross_device_utils.py\", line 22, in <module>\n",
            "    from tensorflow.python.distribute import values as value_lib\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/values.py\", line 23, in <module>\n",
            "    from tensorflow.python.distribute import distribute_lib\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 205, in <module>\n",
            "    from tensorflow.python.data.ops import dataset_ops\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/__init__.py\", line 21, in <module>\n",
            "    from tensorflow.python.data import experimental\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/__init__.py\", line 98, in <module>\n",
            "    from tensorflow.python.data.experimental import service\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/service/__init__.py\", line 419, in <module>\n",
            "    from tensorflow.python.data.experimental.ops.data_service_ops import distribute\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\", line 26, in <module>\n",
            "    from tensorflow.python.data.ops import dataset_ops\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 34, in <module>\n",
            "    from tensorflow.python.data.ops import iterator_ops\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 45, in <module>\n",
            "    from tensorflow.python.training.saver import BaseSaverBuilder\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py\", line 50, in <module>\n",
            "    from tensorflow.python.training import py_checkpoint_reader\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\", line 19, in <module>\n",
            "    from tensorflow.python.util._pywrap_checkpoint_reader import CheckpointReader\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py\", line 42, in tf\n",
            "    from tensorboard.compat import notf  # noqa: F401\n",
            "ImportError: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "AttributeError: _ARRAY_API not found\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py\", line 42, in tf\n",
            "    from tensorboard.compat import notf  # noqa: F401\n",
            "ImportError: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/2024-final-letadylka-prochazka-belohlavek/src/main.py\", line 29, in <module>\n",
            "    from torch.utils.tensorboard.writer import SummaryWriter\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py\", line 12, in <module>\n",
            "    from .writer import FileWriter, SummaryWriter  # noqa: F401\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\", line 19, in <module>\n",
            "    from ._embedding import get_embedding_info, make_mat, make_sprite, make_tsv, write_pbtxt\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/_embedding.py\", line 10, in <module>\n",
            "    _HAS_GFILE_JOIN = hasattr(tf.io.gfile, \"join\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 65, in __getattr__\n",
            "    return getattr(load_once(self), attr_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 97, in wrapper\n",
            "    cache[arg] = f(arg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 50, in load_once\n",
            "    module = load_fn()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py\", line 45, in tf\n",
            "    import tensorflow\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\", line 47, in <module>\n",
            "    from tensorflow._api.v2 import __internal__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 11, in <module>\n",
            "    from tensorflow._api.v2.__internal__ import distribute\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py\", line 8, in <module>\n",
            "    from tensorflow._api.v2.__internal__.distribute import combinations\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py\", line 8, in <module>\n",
            "    from tensorflow.python.distribute.combinations import env # line: 456\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/combinations.py\", line 33, in <module>\n",
            "    from tensorflow.python.distribute import collective_all_reduce_strategy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 25, in <module>\n",
            "    from tensorflow.python.distribute import cross_device_ops as cross_device_ops_lib\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/cross_device_ops.py\", line 28, in <module>\n",
            "    from tensorflow.python.distribute import cross_device_utils\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/cross_device_utils.py\", line 22, in <module>\n",
            "    from tensorflow.python.distribute import values as value_lib\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/values.py\", line 23, in <module>\n",
            "    from tensorflow.python.distribute import distribute_lib\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 205, in <module>\n",
            "    from tensorflow.python.data.ops import dataset_ops\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/__init__.py\", line 21, in <module>\n",
            "    from tensorflow.python.data import experimental\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/__init__.py\", line 98, in <module>\n",
            "    from tensorflow.python.data.experimental import service\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/service/__init__.py\", line 419, in <module>\n",
            "    from tensorflow.python.data.experimental.ops.data_service_ops import distribute\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\", line 26, in <module>\n",
            "    from tensorflow.python.data.ops import dataset_ops\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 34, in <module>\n",
            "    from tensorflow.python.data.ops import iterator_ops\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 45, in <module>\n",
            "    from tensorflow.python.training.saver import BaseSaverBuilder\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py\", line 50, in <module>\n",
            "    from tensorflow.python.training import py_checkpoint_reader\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\", line 19, in <module>\n",
            "    from tensorflow.python.util._pywrap_checkpoint_reader import CheckpointReader\n",
            "SystemError: initialization of _pywrap_checkpoint_reader raised unreported exception\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download files from our Git repository\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NVIUhKxST-5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the network.py file from GitHub\n",
        "!wget -q https://raw.githubusercontent.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/main/src/environment.py\n",
        "\n",
        "!wget -q https://raw.githubusercontent.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/main/src/eval.py\n",
        "\n",
        "!wget -q https://raw.githubusercontent.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/main/src/network.py\n",
        "\n",
        "!wget -q https://raw.githubusercontent.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/main/src/routing.py\n",
        "\n",
        "!wget -q https://raw.githubusercontent.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/main/src/policy.py\n",
        "\n",
        "!wget -q https://raw.githubusercontent.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/main/src/model.py\n",
        "\n",
        "!wget -q https://raw.githubusercontent.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/main/src/wrapper.py\n",
        "\n",
        "!wget -q https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/tree/main/data/adj_mat_fixed.npy\n",
        "\n",
        "!wget -q https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/tree/main/data/dist_mat_fixed.npy\n",
        "\n",
        "!wget -q https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/tree/main/data/sparse_points_fixed.json\n",
        "\n",
        "!wget -q https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/src/config.yaml\n",
        "\n",
        "!pip install pyyaml"
      ],
      "metadata": {
        "id": "zmYi3QPlUsoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the files"
      ],
      "metadata": {
        "id": "0W-MTeWPeJ_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "\n",
        "adj_mat = np.load('adj_mat_fixed.npy')\n",
        "dist_mat = np.load('dist_mat_fixed')\n",
        "with open('sparse_points_fixed.json', 'r') as file:\n",
        "    sparse_points = json.load(file)\n",
        "\n",
        "import yaml\n",
        "\n",
        "with open('config.yaml', 'r') as file:\n",
        "    config = yaml.safe_load(file)"
      ],
      "metadata": {
        "id": "TLoOiq8GdMIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main.py adjusted for evaluation only"
      ],
      "metadata": {
        "id": "EkF-to75frGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from network import Network\n",
        "from environment import reset_and_get_sizes\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import traceback\n",
        "import json\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "from model import DGN, MLP, CommNet, NetMon, DQN\n",
        "from routing import Routing\n",
        "\n",
        "from wrapper import NetMonWrapper\n",
        "from policy import ShortestPath, EpsilonGreedy\n",
        "from pathlib import Path\n",
        "from torch.utils.tensorboard.writer import SummaryWriter\n",
        "from util import (\n",
        "    dim_str_to_list,\n",
        "    filter_dict,\n",
        "    get_state_dict,\n",
        "    interpolate_model,\n",
        "    load_state_dict,\n",
        "    set_attributes,\n",
        "    set_seed,\n",
        ")\n",
        "from eval import evaluate\n",
        "import sys\n",
        "from torch.utils.tensorboard.writer import SummaryWriter"
      ],
      "metadata": {
        "id": "WU108ndNfqQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize distance matrix\n",
        "dist_mat[adj_mat==0] = 0\n",
        "min = 1000\n",
        "for i in range(118):\n",
        "    for j in range(118):\n",
        "        if 0< dist_mat[i][j] < min:\n",
        "            min = dist_mat[i][j]\n",
        "max = np.max(dist_mat)\n",
        "\n",
        "new_min = 1\n",
        "new_max = 10\n",
        "dist_mat = ((dist_mat-min)/(max-min))*(new_max-new_min) + new_min\n",
        "\n",
        "config['only_eval']['eval'] = True # overwrite any setting to evaluate only\n",
        "if config['only_eval']['eval']:\n",
        "    assert Path(config['only_eval']['model_path']).exists()\n",
        "    loaded_dict = torch.load(config['only_eval']['model_path'], map_location='cpu')\n",
        "    loaded_model_arg_values = loaded_dict[\"args\"]\n",
        "    loaded_model_arg_values['only_eval'] = {}\n",
        "    loaded_model_arg_values['only_eval']['eval'] = True\n",
        "    loaded_model_arg_values['only_eval']['model_path'] = config['only_eval']['model_path']\n",
        "    config = loaded_model_arg_values\n",
        "    config['evaluation']['episodes'] = 10\n",
        "    config['evaluation']['episode_steps'] = 100\n",
        "    config['training']['mini_batch_size'] = 8\n",
        "    config['training']['sequence_length'] = 5\n",
        "    config['netmon']['iterations'] = 3\n",
        "    config['device'] = 'cpu'\n",
        "    for key in config:\n",
        "        if key == \"device\":\n",
        "            print(f\"{key}:  {config[key]}\")\n",
        "            continue\n",
        "        print(key)\n",
        "        for subkey in config[key]:\n",
        "            print(f\"\\t{subkey}: {config[key][subkey]}\")\n",
        "\n",
        "cbase = config['base']\n",
        "cnetmon = config['netmon']\n",
        "device = config['device']\n",
        "ctar_update = config['target_update']\n",
        "ceval = config['evaluation']\n",
        "ctraining = config['training']\n",
        "ceps = config['epsilon_greedy']\n",
        "\n",
        "# Define network environment\n",
        "network = Network(adj_mat, dist_mat, sparse_points)\n",
        "\n",
        "\n",
        "# Define type of environment\n",
        "env = Routing(network, cbase['n_planes'], cbase['env_var'], adj_mat, dist_mat, k=cbase['n_neighbors'], enable_action_mask=False)\n",
        "\n",
        "# Define activation function\n",
        "activation_function = getattr(F, cbase['activ_f'])\n",
        "\n",
        "# Dynamically resets the environment\n",
        "n_agents, agent_obs_size, n_nodes, node_obs_size = reset_and_get_sizes(env)\n",
        "\n",
        "print(\"Sizes before netmon:\")\n",
        "print(\"Agent observation size: \", agent_obs_size)\n",
        "print(\"Node observation size: \", node_obs_size)\n",
        "\n",
        "# Use NetMon - init is rather long :)\n",
        "netmon = NetMon(node_obs_size,  # 'in_features' in init\n",
        "                cnetmon['dim'],     # 'hidden_features' in init\n",
        "                cnetmon['enc_dim'] , # 'encoder_units' in init\n",
        "                iterations=cnetmon['iterations'],\n",
        "                activation_fn=activation_function,\n",
        "                rnn_type= cnetmon['rnn_type'], rnn_carryover=cnetmon['rnn_carryover'], agg_type=cnetmon['agg_type'],\n",
        "                output_neighbor_hidden=cnetmon['neighbor'], output_global_hidden=cnetmon['global']\n",
        "                ).to(device)    # Move to device\n",
        "\n",
        "\n",
        "# Get observations from the environment\n",
        "summary_node_obs = torch.tensor(env.get_node_observation(), dtype=torch.float32, device=device).unsqueeze(0)\n",
        "summary_node_adj = torch.tensor(env.get_nodes_adjacency(), dtype=torch.float32, device=device).unsqueeze(0)\n",
        "summary_node_agent = torch.tensor(env.get_node_agent_matrix(), dtype=torch.float32, device=device).unsqueeze(0)\n",
        "# Summarizes our current model - just to have it somewhere\n",
        "netmon_summary = netmon.summarize(summary_node_obs, summary_node_adj, summary_node_agent)\n",
        "node_state_size = netmon.get_state_size()\n",
        "\n",
        "node_aux_size = 0 if env.get_node_aux() is None else len(env.get_node_aux()[0]) # = n_waypoints\n",
        "\n",
        "# Now we wrap the whole netmon class with a Wrapper - agents will use observations from netmon\n",
        "env = NetMonWrapper(env, netmon, cnetmon['start_up_iters'])\n",
        "_, agent_obs_size, _, _ = reset_and_get_sizes(env)  # Observation length\n",
        "\n",
        "print(\"Sizes after netmon:\")\n",
        "print(f\"Node state size: {node_state_size}\")        # 256\n",
        "print(f\"Agent observation size with netmon: {agent_obs_size}\")  # 3263\n",
        "print(f\"Node auxiliary size: {node_aux_size}\")      # 0\n",
        "\n",
        "\n",
        "\n",
        "# In_features are 'agent_obs_size'\n",
        "# 'env.action_space.n' is equal to the number of neighbors - choices, 'num_actions' in DGN definition\n",
        "cdgn = config['dgn']\n",
        "if cbase['model_type'] == \"dgn\":\n",
        "    model = DGN(agent_obs_size, cdgn['hidden_dim'], env.action_space.n, cdgn['heads'], cdgn['att_layers'], activation_function, cdgn['kv_values']).to(device)\n",
        "elif cbase['model_type'] == \"comm_net\":\n",
        "    ccom_net = config['commnet']\n",
        "    model = CommNet(agent_obs_size, cdgn['hidden_dim'], env.action_space.n, comm_rounds=ccom_net['comm_rounds'], activation_fn=activation_function).to(device)\n",
        "elif cbase['model_type'] == \"dqn\":\n",
        "    model = DQN(agent_obs_size, cdgn['hidden_dim'], env.action_space.n, activation_function).to(device)\n",
        "else:\n",
        "    raise ValueError(\"Invalid model type\")\n",
        "\n",
        "\n",
        "# print(config)\n",
        "# Load paramters of model for quick evaluation\n",
        "if config['only_eval']['eval']:\n",
        "    assert Path(config['only_eval']['model_path']).exists()\n",
        "    load_state_dict(\n",
        "        torch.load(config['only_eval']['model_path'], map_location=device),\n",
        "        model,\n",
        "        netmon,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "ZlsGA7KMglIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_tar = copy.deepcopy(model).to(device)     # Create a deep copy of the current model == DGN\n",
        "\n",
        "model = model.load_state_dict(torch.load('model.pth'))\n",
        "model_has_state = hasattr(model, \"state\")\n",
        "aux_model = None\n",
        "\n",
        "policy = EpsilonGreedy(env, model, env.action_space.n, epsilon=ceps['epsilon'], step_before_train=ctraining['step_before_train'], epsilon_update_freq=ceps['epsilon_update_freq'], epsilon_decay=ceps['epsilon_decay'])\n",
        "\n",
        "if config['only_eval']['eval']:\n",
        "    model.eval()\n",
        "    netmon.eval()\n",
        "    print(\"loaded\")\n",
        "    print(\"Performing Evaluation\")\n",
        "    metrics = evaluate(env, policy, ceval['episodes'], ceval['episode_steps'],\n",
        "                      True, \"eval_dict\", ceval['output_detailed'], ceval['output_node_state_aux']\n",
        "                      )\n",
        "    print(json.dumps(metrics, indent=4, sort_keys=True, default=str))\n",
        "    sys.exit(0)"
      ],
      "metadata": {
        "id": "di5NpByHiLPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "comment = \"_\"\n",
        "if hasattr(env, \"env_var\"):\n",
        "    comment += f\"R{env.env_var.value}\"\n",
        "comment += \"_netmon\"\n",
        "writer = SummaryWriter(comment=comment)\n",
        "print(\"Performing evaluation:\")\n",
        "metrics = evaluate(\n",
        "    env,\n",
        "    policy,\n",
        "    ceval['episodes'],\n",
        "    ceval['episode_steps'],\n",
        "    Path(writer.get_logdir()) /\"eval\",\n",
        "    ceval['output_detailed'],\n",
        "    ceval['output_node_state_aux']\n",
        ")\n",
        "paths_to_save = env.save_paths()\n",
        "print(json.dumps(metrics, indent = 4, sort_keys=True, default=str))\n",
        "\n",
        "for plane in env.planes:\n",
        "    print(plane.paths)\n",
        "\n",
        "env.plot_trajectory()"
      ],
      "metadata": {
        "id": "TLSBYNhNihQg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}