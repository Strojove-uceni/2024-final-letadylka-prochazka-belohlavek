{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE OF CONTENTS**"
      ],
      "metadata": {
        "id": "xgl4XoSJHuxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Introduction](#scrollTo=Mey2sVO3w06m)\n",
        "\n",
        ">>[Abstract](#scrollTo=Mey2sVO3w06m)\n",
        "\n",
        ">>[How to Run the Project](#scrollTo=Mey2sVO3w06m)\n",
        "\n",
        ">[Overview of the Used Machine Learning Techniques](#scrollTo=ZDfbtUw36j_1)\n",
        "\n",
        ">>[Architectures](#scrollTo=ZDfbtUw36j_1)\n",
        "\n",
        ">>>[Disclamer](#scrollTo=ZDfbtUw36j_1)\n",
        "\n",
        ">>[Q-values prediction](#scrollTo=ZDfbtUw36j_1)\n",
        "\n",
        ">>[Classical MLP](#scrollTo=ZDfbtUw36j_1)\n",
        "\n",
        ">>[Attention Model](#scrollTo=HU4Z78Kv9hiW)\n",
        "\n",
        ">>[DGN](#scrollTo=_s_7VNFm91w8)\n",
        "\n",
        ">>[DQN](#scrollTo=G5-kiKLj98yv)\n",
        "\n",
        ">>[CommNet](#scrollTo=VK8i3VvG-Bg-)\n",
        "\n",
        ">>[State Aggregation](#scrollTo=AGSMkNTI-KIH)\n",
        "\n",
        ">>>[SUM](#scrollTo=AGSMkNTI-KIH)\n",
        "\n",
        ">>>[GCN](#scrollTo=7S-b66SX-TyK)\n",
        "\n",
        ">[Selecting Parameters](#scrollTo=1Ei12632gg3Z)\n",
        "\n",
        ">>[Common Parameters in the Sweep](#scrollTo=1Ei12632gg3Z)\n",
        "\n",
        ">>[CommNet specific](#scrollTo=1Ei12632gg3Z)\n",
        "\n",
        ">>[DQN, DGN specific](#scrollTo=1Ei12632gg3Z)\n",
        "\n",
        ">[Advice for Parameter Selection](#scrollTo=OhkNKLaYlGRj)\n",
        "\n",
        ">>[CommNet settings](#scrollTo=OhkNKLaYlGRj)\n",
        "\n",
        ">>[DQN, DGN settings](#scrollTo=OhkNKLaYlGRj)\n",
        "\n",
        ">>[Aggregation Type](#scrollTo=OhkNKLaYlGRj)\n",
        "\n",
        ">[Our results](#scrollTo=S8nU4WQLwJI7)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "Ud4C609CHs-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Authors: Michal Bělohlávek, Tomáš Procházka\n",
        "\n",
        "# Introduction\n",
        "Welcome to the demo file, where one can run the project with zero effort and see the results and visualistaion for themselves. While this is an easy and plesant way to enjoy this Neural Net, we strongly urge anyone who visits this demo to run the project as it was intended, expand upon it and improve it.\n",
        "\n",
        "## Abstract\n",
        "This project was created and submited as the final semestral project for the Machine Learning 2 class on FNSPE CTU. This project concerns itself with reinforcement learning for multiple agents controlled by a single neural net in a graph environment. The core aim of this project is to provide a neural network solution that efficiently navigates multiple planes along a fully connected graph with the goal of estimating the shortest path, while avoiding collisions of planes. We implemented an enhanced version of a classical replay buffer that samples experiences based on the predicted future reward to assist the learning process. We also added regularization techniques, since the dimensionality of our problem is much larger compared to the on in the repository we cite.\n",
        "\n",
        "## How to Run the Project\n",
        "For those who decide to download the project and run the training on their PC, please beware of the configurations. A basic setup is present in /data as demo_config.yaml and runs on CPU.\n",
        "\n",
        "Setting up capacity, minibatch_size or sequence_length too high may result in freezing the computer.\n",
        "\n",
        "Most hyperparameters may be changed in the config.yaml file. If you intend to do your own sweeps on weights and biases, we have also uploaded a version of the main file wandb_main.py that supports sweep configuration.\n",
        "\n",
        "If you however decide to only run the project in this demo file, note that any pre-trained models are too large to upload to the GitHub repo directly, so the training will be done from scratch here. The training will use the demo_confing with small number of steps and generally \"low\" settings, so taht the training can be completed in reasonable amount of time. Therefore, one should expect very poor results compared to the results we present at the end of this notebook. Note that we need to install specific versions of many libraries that are compatible, this may take a while.\n",
        "\n",
        "The Runtime may encounter an error with tensorflow, simply restart Runtime and run the following cells and training will start. The training takes about 5 minutes. Sometimes Google Colab fails to connect to GitHub. The error you may encounter is that it fails to find the directory. To solve this, simply keep deleting the runtime until it works. If you encounter any errors about pip dependency, ignore it and proceed.\n",
        "\n",
        "**To see the training and results, simply run the following code boxes.**\n"
      ],
      "metadata": {
        "id": "Mey2sVO3w06m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek"
      ],
      "metadata": {
        "id": "9mdfd6guYlQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/2024-final-letadylka-prochazka-belohlavek/\n",
        "!pip install -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt"
      ],
      "metadata": {
        "id": "XVMOpK9Tgo63",
        "outputId": "374b575d-5545-456b-c61c-e4ef2fab4e1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/2024-final-letadylka-prochazka-belohlavek\n",
            "Collecting gymnasium==1.0.0 (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 1))\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting matplotlib==3.9.3 (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2))\n",
            "  Downloading matplotlib-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: networkx==3.4.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 3)) (3.4.2)\n",
            "Collecting numpy==2.2.0 (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 4))\n",
            "  Downloading numpy-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==2.2.3 (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 5))\n",
            "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 6)) (6.0.2)\n",
            "Collecting torch==2.5.0 (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torch_geometric==2.6.1 (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9))\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.67.0 (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 10))\n",
            "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb==0.19.0 (from -r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11))\n",
            "  Downloading wandb-0.19.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.0.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.0.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 1)) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium==1.0.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 1))\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2)) (4.55.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2)) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 5)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 5)) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8)) (3.16.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8)) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8))\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8)) (1.13.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (3.11.10)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (5.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (2.32.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (4.25.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (2.10.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (75.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (4.0.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.1->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 9)) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 8)) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.19.0->-r /content/2024-final-letadylka-prochazka-belohlavek/requirements.txt (line 11)) (5.0.1)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.19.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, triton, tqdm, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gymnasium, wandb, nvidia-cusolver-cu12, matplotlib, torch_geometric, torch\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.6\n",
            "    Uninstalling tqdm-4.66.6:\n",
            "      Successfully uninstalled tqdm-4.66.6\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: wandb\n",
            "    Found existing installation: wandb 0.18.7\n",
            "    Uninstalling wandb-0.18.7:\n",
            "      Successfully uninstalled wandb-0.18.7\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.8.0\n",
            "    Uninstalling matplotlib-3.8.0:\n",
            "      Successfully uninstalled matplotlib-3.8.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.2.0 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "langchain 0.3.10 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.0 which is incompatible.\n",
            "pytensor 2.26.4 requires numpy<2,>=1.17.0, but you have numpy 2.2.0 which is incompatible.\n",
            "tensorflow 2.17.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.2.0 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.0 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.5.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed farama-notifications-0.0.4 gymnasium-1.0.0 matplotlib-3.9.3 numpy-2.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pandas-2.2.3 torch-2.5.0 torch_geometric-2.6.1 tqdm-4.67.0 triton-3.1.0 wandb-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow, pandas\n",
        "!pip install numpy==1.26.4, tensorboard==2.18.0, pandas==2.2.2\n",
        "!conda install -c conda-forge cudatoolkit cudnn"
      ],
      "metadata": {
        "id": "SVFsINSti3Vg",
        "outputId": "7b259d77-134e-42d9-d41a-19491daa54fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'tensorflow,': Expected end or semicolon (after name and no valid version specifier)\n",
            "    tensorflow,\n",
            "              ^\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard==2.18.0\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pandas==2.2.2\n",
            "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (1.68.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0) (3.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2024.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard==2.18.0) (3.0.2)\n",
            "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, tensorboard, pandas\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.0\n",
            "    Uninstalling numpy-2.2.0:\n",
            "      Successfully uninstalled numpy-2.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.3\n",
            "    Uninstalling pandas-2.2.3:\n",
            "      Successfully uninstalled pandas-2.2.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires tensorboard<2.18,>=2.17, but you have tensorboard 2.18.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 pandas-2.2.2 tensorboard-2.18.0\n",
            "/bin/bash: line 1: conda: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the main.py script with the modified config\n",
        "%cd  /content/2024-final-letadylka-prochazka-belohlavek\n",
        "!python /content/2024-final-letadylka-prochazka-belohlavek/src/main.py --demo_config data/demo_config.yaml"
      ],
      "metadata": {
        "id": "cIBJLw9yglGC",
        "outputId": "ed64b6a7-827a-4a23-80b4-cdd30f3ef98f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/2024-final-letadylka-prochazka-belohlavek\n",
            "2024-12-13 14:24:19.342974: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-13 14:24:19.371717: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-13 14:24:19.379803: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-13 14:24:19.404203: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-13 14:24:21.480160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "seed for the buffer is:  595292149\n",
            "/usr/local/lib/python3.10/dist-packages/networkx/utils/backends.py:1355: FutureWarning: \n",
            "\n",
            "shortest_path will return an iterator that yields\n",
            "(node, path) pairs instead of a dictionary when source\n",
            "and target are unspecified beginning in version 3.5\n",
            "\n",
            "To keep the current behavior, use:\n",
            "\n",
            "\tdict(nx.shortest_path(G))\n",
            "  return self._call_with_backend(backend_name, args, kwargs)\n",
            "Sizes before netmon:\n",
            "Agent observation size:  1458\n",
            "Node observation size:  1200\n",
            "Sizes after netmon:\n",
            "Node state size: 256\n",
            "Agent observation size with netmon: 1714\n",
            "Node auxiliary size: 118\n",
            "In features are:  512\n",
            "runs/Dec13_14-24-24_954e8eefd19a_R2_netmon\n",
            "Training with parameters\n",
            "Model type: DQN\n",
            "NetMon Module\n",
            "+----------------------------------+---------------------------------------------+------------------------+-----------+\n",
            "| Layer                            | Input Shape                                 | Output Shape           | #Param    |\n",
            "|----------------------------------+---------------------------------------------+------------------------+-----------|\n",
            "| NetMon                           | [1, 118, 1200], [1, 118, 118], [1, 118, 10] | [1, 10, 256]           | 2,084,480 |\n",
            "| ├─(encode)MLP                    | [118, 1200]                                 | [118, 128]             | 1,820,288 |\n",
            "| │    └─(dropout)Dropout          | [118, 1024]                                 | [118, 1024]            | --        |\n",
            "| │    └─(linear_layers)ModuleList | --                                          | --                     | 1,820,288 |\n",
            "| │    │    └─(0)Linear            | [118, 1200]                                 | [118, 1024]            | 1,229,824 |\n",
            "| │    │    └─(1)Linear            | [118, 1024]                                 | [118, 512]             | 524,800   |\n",
            "| │    │    └─(2)Linear            | [118, 512]                                  | [118, 128]             | 65,664    |\n",
            "| ├─(aggregate)SimpleAggregation   | [1, 118, 128], [1, 118, 118]                | [1, 118, 128]          | --        |\n",
            "| ├─(rnn_obs)LSTMCell              | [118, 128]                                  | [118, 128], [118, 128] | 132,096   |\n",
            "| ├─(rnn_update)LSTMCell           | [118, 128]                                  | [118, 128], [118, 128] | 132,096   |\n",
            "+----------------------------------+---------------------------------------------+------------------------+-----------+\n",
            "> Aggregation Type: sum\n",
            "> RNN Type: lstm\n",
            "> Carryover: True\n",
            "> Iterations: 4\n",
            "> Readout: local + global agg\n",
            "Routing environment with parameters\n",
            "> Network: 118 nodes\n",
            "> Number of planes: 10\n",
            "> Environment variant: WITH_K_NEIGHBORS\n",
            "> Number of considered neighbors (k): 4\n",
            "> Action mask: False            \n",
            "▲ environment is wrapped with NetMon (graph obs)\n",
            "Episode: 1  step: 0k  reward: 0.75  delays: 90.91  delays_arrived: 82.00  spr: 28.26  looped: 2.92  throughput: 0.01  dropped: 0.00  blocked: 0.00 | BEST\n",
            "Episode: 2  step: 0k  reward: 0.85  delays: 83.33  delays_arrived: 58.50  spr: 18.45  looped: 3.12  throughput: 0.02  dropped: 0.00  blocked: 0.00 | BEST\n",
            "Episode: 3  step: 0k  reward: 0.85  delays: 83.33  delays_arrived: 51.00  spr: 10.99  looped: 2.92  throughput: 0.02  dropped: 0.00  blocked: 0.00  loss_aux: 170.29  q_values: 0.75  q_target: 0.95  loss: 6.32\n",
            "Episode: 4  step: 0k  reward: 0.96  delays: 90.91  delays_arrived: 38.00  spr: 25.00  looped: 3.09  throughput: 0.01  dropped: 0.00  blocked: 0.00  loss_aux: 68.60  q_values: 0.94  q_target: 1.02  loss: 4.50 | BEST\n",
            "Episode: 5  step: 0k  reward: 0.80  delays: 90.91  delays_arrived: 9.00  spr: 3.23  looped: 3.74  throughput: 0.01  dropped: 0.00  blocked: 0.00  loss_aux: 51.29  q_values: 1.05  q_target: 1.05  loss: 4.43\n",
            "Episode: 6  step: 1k  reward: 1.05  delays: 83.33  delays_arrived: 31.00  spr: 11.85  looped: 3.08  throughput: 0.02  dropped: 0.00  blocked: 0.02  loss_aux: 49.59  q_values: 1.04  q_target: 1.07  loss: 4.39 | BEST\n",
            "Episode: 7  step: 1k  reward: 1.14  delays: 71.43  delays_arrived: 43.25  spr: 12.64  looped: 2.68  throughput: 0.04  dropped: 0.00  blocked: 0.00  loss_aux: 44.22  q_values: 1.20  q_target: 1.35  loss: 4.83 | BEST\n",
            "Episode: 8  step: 1k  reward: 0.99  delays: 83.33  delays_arrived: 71.00  spr: 24.52  looped: 2.82  throughput: 0.02  dropped: 0.00  blocked: 0.01  loss_aux: 34.76  q_values: 1.37  q_target: 1.36  loss: 4.68\n",
            "Episode: 9  step: 1k  reward: 0.80  delays: 71.43  delays_arrived: 59.75  spr: 24.93  looped: 3.18  throughput: 0.04  dropped: 0.00  blocked: 0.01  loss_aux: 19.24  q_values: 1.46  q_target: 1.62  loss: 4.78\n",
            "Episode: 10  step: 1k  reward: 0.79  delays: 90.91  delays_arrived: 53.00  spr: 11.16  looped: 3.01  throughput: 0.01  dropped: 0.00  blocked: 0.03  loss_aux: 14.16  q_values: 1.51  q_target: 1.52  loss: 4.36\n",
            "Clean exit\n",
            "Performing evaluation:\n",
            "Plane  6  reached the target.\n",
            "Plane  8  reached the target.\n",
            "Plane  8  reached the target.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/2024-final-letadylka-prochazka-belohlavek/src/main.py\", line 602, in <module>\n",
            "    paths_to_save = env.save_paths()\n",
            "  File \"/content/2024-final-letadylka-prochazka-belohlavek/src/wrapper.py\", line 29, in __getattr__\n",
            "    return getattr(self.env, name)  # If not found in the Netmon, look into the environment\n",
            "AttributeError: 'Routing' object has no attribute 'save_paths'\n",
            "An exception was raised during evaluation (see above).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview of the Used Machine Learning Techniques\n",
        "## Architectures\n",
        "\n",
        "<span><font color=\"green;\">\n",
        "###Disclamer\n",
        "\n",
        "**The following code is used as an illustration only, to see the functionalities of the code directly, visit the /src file on our GitHub repo. This approach was taken because it is not feasible to copy the whole code into Colab.**\n",
        "</font></span>\n",
        "\n",
        "(Or maybe it would be feasible but that would be an extreme violation of our hard work.)\n",
        "\n",
        "This file contains the description of the main architectures used within this project. Below we provide a detailed description for all of them.\n",
        "\n",
        "First we describe models that were used for Q-values predictions:\n",
        "\n",
        "    - DGN\n",
        "    - DQN\n",
        "    - Comm_net,\n",
        "    \n",
        "then we go over the methods that were used to aggregate hidden graph representations:\n",
        "\n",
        "    - SUM\n",
        "    - GCN\n",
        "\n",
        "and lastly we describe the **NetMon** class, that was originally provided by the authors.\n",
        "\n",
        "## Reinforcement Learning\n",
        "First, let's introduce the concept of reinforcement learning with multiple agents. Each agent observes only a partial view of some environment and state information. Reinforcement Learning (RL) is a machine learning approach, where agents learn to make sequential decisions by interacting with an environment. Each agent hold a local and/or global observation of its environment. Through a process of trial and error, the agent receives rewards or penalties for their actions, enabling it to discover an optimal policy for achieving specific goals. In our case, for example, we used Epsilon-Greedy policy, where at the beginning the agent makes random decisions to explore the environment and allow itself to learn the interactions. Then it gradually transfers to learned behavior.\n",
        "\n",
        "In the context of multiple agents, Recurrent Neural Networks (RNNs) play a critical role in handling temporal dependencies. For example, in multi-agent RL, RNNs can be used to model and predict the behavior of agents based on sequences of past experiences, enabling better coordination and communication between agents. By maintaining hidden states that encapsulate the history of interactions, RNNs empowers agents to adapt to dynamic environments and collaborate effectively.\n",
        "\n",
        "Our work prouds itself amongst other things on the replay_buffer that significantly improved the prediction of paths that lead to future reward. A replay buffer is a key component in reinforcement learning that stores past experiences, typically in the form of state, action, reward, and next state tuples. For RNNs, which rely on sequential dependencies, replay buffers are particularly important as they allow the agent to learn from diverse trajectories while maintaining temporal coherence. By sampling batches of sequences instead of independent transitions, the replay buffer ensures that the RNN captures meaningful patterns over time, improving its ability to model long-term dependencies. Additionally, we implemented a version of replay buffer that samples the batch sequences based on maximizing the td_error, hence the mean square error between predicted future and immediate rewards. This allows the agents to learn and prioritize paths that lead to targets as that is the place of the most future reward.\n",
        "\n",
        "## Q-values Prediction\n",
        "We use Q-Net for q_value predictions, a reinforcement learning technique that assigns values to each future action for one step into the future based on agent's observations (state). In this particular setting, the Q-Net predicts the reward for each edge the agent could take at any given step. We implemented a node mask and generalized the setting to fit grahps with variable edge count for each node. The algorithm in Q-Net implements dynamic programming weighted by the learning rate hyperparameter.\n",
        "\n",
        "$$Q_{target} = (1-l)*Q_{now}+l * E[R_{t+1}(a_{t+1}, s_{t+1}) + \\gamma * max_{a_{t+1}}Q_{next}(a_{t+1}, s_{t+1})| s_t],$$\n",
        "\n",
        "where $R_{t+1}(a_{t+1}, s_{t+1})$ is sampled from batch of experiences and $l$ signifies learning rate. From this formulation, we can se that sampling the batch indices that maximize the td_error, we essentially grow the $Q_{now}$ values for future steps.\n",
        "\n",
        "The goal of each agent is to maximize the expected future reward weighted by the gamma (discount) factor\n",
        "\n",
        "$$max_{(a)_{t_0}^T} E[\\sum_{t=t_0}^T \\gamma^{t-t_0}R_{t}(a_t, s_t)].$$\n",
        "\n",
        "## Classical MLP\n",
        "MLP is a feed forward network that passes the input thourgh many linear layers with activation functions. In our case, we used leaky-ReLU as the activation function. It is also possible to modify this setting to for example GeLU in the config.yaml file but we should points out that this may lead to the agent learning to take forbidden edges, that are subsequently masked leading to insufficient gradient flow. This approach has not been explored in this project. Dropout is included for regularization."
      ],
      "metadata": {
        "id": "ZDfbtUw36j_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    This is the underlying module for all used models within this work.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, mlp_units, activation_fn, activation_on_output = True):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.activation = activation_fn\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "\n",
        "        self.linear_layers = nn.ModuleList() # Storage for L layers\n",
        "        previous_units = in_features\n",
        "\n",
        "        # Transform units into a list\n",
        "        if isinstance(mlp_units, int):\n",
        "            mlp_units = [mlp_units]\n",
        "\n",
        "        # Create a chain of layers\n",
        "        for units in mlp_units:\n",
        "            self.linear_layers.append(nn.Linear(previous_units, units))\n",
        "            previous_units = units\n",
        "\n",
        "        self.out_features = previous_units\n",
        "        self.activation_on_ouput = activation_on_output\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Inter layers\n",
        "        for module in self.linear_layers[:-1]:\n",
        "            x = module(x)\n",
        "            if self.activation is not None:\n",
        "                x = self.activation(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Pass through the last layer\n",
        "        x = self.linear_layers[-1](x)\n",
        "        if self.activation_on_ouput:\n",
        "            x = self.activation(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "7gDhEfU66-5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Model"
      ],
      "metadata": {
        "id": "HU4Z78Kv9hiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttModel(nn.Module):\n",
        "    \"\"\"\n",
        "        Basic attention model with with masking and scaling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, k_features, v_features, out_features, num_heads, activation_fn, vkq_activation_fn):\n",
        "        super(AttModel, self).__init__()\n",
        "\n",
        "\n",
        "        self.k_features = k_features\n",
        "        self.v_features = v_features\n",
        "        self.num_heads = num_heads      # Number of attention heads\n",
        "\n",
        "        self.fc_v = nn.Linear(in_features, v_features * num_heads)  # Transforming input features into Values for attention\n",
        "        self.fc_k = nn.Linear(in_features, k_features * num_heads)  # Transforming input features into Keys for attention\n",
        "        self.fc_q = nn.Linear(in_features, k_features * num_heads)  # Transforming input values into Queries for attention\n",
        "\n",
        "        self.fc_out = nn.Linear(v_features * num_heads, out_features)   # Transforms the outputs from all attention heads into output dimension\n",
        "\n",
        "        self.activation = activation_fn\n",
        "        self.vkq_activation = vkq_activation_fn     # Activation function that can be applied into Values, Keys, Queries\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        Defining the scaling factor for attention as 1/ sqrt(d_k), this is the same as the publishing paper \"Attention is All You Need\".\n",
        "        This is done for the purpose of reducing the gradient so it does not become too large. Later you will see that without it, the dot product\n",
        "        would grow too large without the scaling\n",
        "        \"\"\"\n",
        "        self.attention_scale = 1 / (k_features **0.5)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, x, mask):\n",
        "        batch_size, num_agents = x.shape[0], x.shape[1]\n",
        "\n",
        "        \"\"\"\n",
        "        The code below does the following:\n",
        "            - a linear mapping is applied on the inputs to obtain Values, Keys, Queries\n",
        "            - the Values, Keys, Queries are then reshaped to separate the different attention heads of the model\n",
        "            :reshape: will result in (batch_size, num_agents, num_heads, features_per_head)\n",
        "\n",
        "        Visual representation:\n",
        "            Input x\n",
        "            |\n",
        "            [Linear Layers] -> V, Q, K\n",
        "            |\n",
        "            [Optional Activation] (vkq_activation_fn)\n",
        "            |\n",
        "            [Reshape for Multi-Head]\n",
        "            |\n",
        "            [Transpose for Heads]\n",
        "            |\n",
        "            [Compute Attention Weights (Dot Product, Scale, Mask, Softmax)]\n",
        "            |\n",
        "            [Apply Attention to Values]\n",
        "            |\n",
        "            [Skip Connection]\n",
        "            |\n",
        "            [Transpose and Concatenate Heads]\n",
        "            |\n",
        "            [Final Linear Layer and Activation]\n",
        "            |\n",
        "            Output\n",
        "        \"\"\"\n",
        "\n",
        "        v = self.fc_v(x).view(batch_size, num_agents, self.num_heads, self.v_features)\n",
        "        q = self.fc_q(x).view(batch_size, num_agents, self.num_heads, self.k_features)\n",
        "        k = self.fc_k(x).view(batch_size, num_agents, self.num_heads, self.k_features)\n",
        "\n",
        "        if self.vkq_activation is not None:\n",
        "            v = self.vkq_activation(v)\n",
        "            q = self.vkq_activation(q)\n",
        "            k = self.vkq_activation(k)\n",
        "\n",
        "        # We rearrange the tensors to shape (batch_size, num_heads, num_agents, features_per_head)\n",
        "        # This is done so we can perform batch multiplication over the batch size and heads\n",
        "        q, k, v = q.transpose(1,2), k.transpose(1,2), v.transpose(1,2)\n",
        "\n",
        "        # Add head axis (we are keeping the same mask for all attention heads)\n",
        "        mask = mask.unsqueeze(1)    # (batch_size, 1, num_agents, num_agents) (1,1,20,20)\n",
        "\n",
        "        \"\"\"\n",
        "        The attention is calculated as a dot product of all queries with all keys,\n",
        "            while scaling it with the attention scale so it does not explode.\n",
        "            - q is of shape             (batch_size, num_heads, num_agents, features_per_head)\n",
        "            - k transposed is of shape  (batch_size, num_heads, features_per_head, num_agents)\n",
        "            - the multiplication result is of shape (batch_size, num_heads, num_agents, num_agents)\n",
        "        :masked_fill sets positions where mask == 0 to a large negative value - removes them from the attention computation practically\n",
        "        \"\"\"\n",
        "\n",
        "        att_weights = torch.matmul(q, k.transpose(2, 3)) * self.attention_scale\n",
        "        att = att_weights.masked_fill(mask==0, -1e9)\n",
        "        att = F.softmax(att, dim=-1)    # Softmax is applied along the last dimension to obtain normalized attention probabilities\n",
        "        att = self.dropout(att)\n",
        "\n",
        "        # Now we combine the Values with respect to the attention we just computed\n",
        "        \"\"\"\n",
        "            - att is of shape (batch_size, num_heads, num_agents, num_agents)\n",
        "            - v is of shape (batch_size, num_heads, num_agents, v_features)\n",
        "            - the multiplication result is of shape (batch_size, num_heads, num_agents, v_features)\n",
        "        \"\"\"\n",
        "        out = torch.matmul(att, v)\n",
        "\n",
        "        # We add a skip connection\n",
        "        out  = torch.add(out, v)    # This additionally promotes gradient flow and mitigates vanishing gradient\n",
        "\n",
        "        # Now \"remove\" the transpose and concatenate all heads together\n",
        "        \"\"\"\n",
        "            - out is of shape (batch_size, num_heads, num_agents, v_features)\n",
        "            - out after transpose is of shape (batch_size, num_agents, num_heads, v_features)\n",
        "            - contiguous() ensures that the tensor is stored in a contiguous chunk of memory so that the reshape for view can happen\n",
        "            - view is used to reshape the tensor to (batch_size, num_agents, v_features), therefore, we flatten the last two dimensions\n",
        "                into a single one (num_heads * v_features)\n",
        "            - final out is of shape  (batch_size, num_agents, num_heads * v_features)\n",
        "        \"\"\"\n",
        "\n",
        "        out = out.transpose(1,2).contiguous().view(batch_size, num_agents, -1)\n",
        "        out = self.activation(self.fc_out(out)) # Linear map into a desired feature dimension\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out, att_weights"
      ],
      "metadata": {
        "id": "Zol8QsIS9nS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DGN\n"
      ],
      "metadata": {
        "id": "_s_7VNFm91w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DGN(nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, mlp_units, num_actions, num_heads, num_attention_layers, activation_fn, kv_values):\n",
        "        super(DGN, self).__init__()\n",
        "\n",
        "        self.encoder = MLP(in_features, mlp_units, activation_fn)\n",
        "        self.att_layers = nn.ModuleList()\n",
        "        hidden_features = self.encoder.out_features\n",
        "\n",
        "        print(\"In features of DGN: \", in_features)\n",
        "        print(\"MLP units are: \", mlp_units)\n",
        "\n",
        "        for _ in range(num_attention_layers):\n",
        "            self.att_layers.append(\n",
        "                AttModel(hidden_features, kv_values, kv_values, hidden_features, num_heads, activation_fn, activation_fn)\n",
        "                                   )\n",
        "\n",
        "        self.q_net = Q_Net(hidden_features * (num_attention_layers + 1), num_actions)\n",
        "\n",
        "        self.att_weights = []\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Additional comment to the function:\n",
        "            - each attention layer refines the representation h by focusing on relevant parts of the input\n",
        "            - by concatenating the representations the feature set for the Q-network is enhanced, consequently making more informed decisions\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        h = self.encoder(x)     # Encodes the input featuers, has a shape of (batch_size, num_agents, hidden_features)\n",
        "        q_input = h     # Initialize the q_input with encoded features\n",
        "        self.att_weights.clear()    # Ensuring that attention weights from previous forward passes do not accumulate\n",
        "\n",
        "        for attention_layer in self.att_layers:\n",
        "            h, att_weight = attention_layer(h, mask)\n",
        "            self.att_weights.append(att_weight)\n",
        "\n",
        "            # Concatenation of outputs\n",
        "            q_input = torch.cat((q_input, h), dim=-1)\n",
        "\n",
        "        # Final q_input is of shape (batch_size, num_agents, hidden_features * (num_attention_layers +1))\n",
        "        q = self.q_net(q_input)\n",
        "\n",
        "        return q    # is of shape (batch_size, num_agents, num_actions)\n"
      ],
      "metadata": {
        "id": "Pgg1EA3J94Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN\n",
        "\n",
        "Deep Q-Learning Network. The encoder MLP transforms input features for generalization purposes that are then passed to the Q_Net to predict the reward for possible actions. Forward action to process the input. While being arguably the simplest model we have, DQN had the best and most consistent performance. This has also been noted by the authors of the graph MARL paper we reference."
      ],
      "metadata": {
        "id": "G5-kiKLj98yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    Introduces simple Deep Feed Forward Neural Network( = MLP) as the encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, mlp_units, num_actions, activation_fn):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.encoder = MLP(in_features, mlp_units, activation_fn)   # Encodes incoming features\n",
        "        self.q_net = Q_Net(self.encoder.out_features, num_actions)  # Outputs Q-values\n",
        "        self.activation = activation_fn\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        batch, agent, features = x.shape\n",
        "        h = self.encoder(x)\n",
        "        q = self.q_net(h)\n",
        "        return q\n"
      ],
      "metadata": {
        "id": "Vx3Sl8CB99oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CommNet"
      ],
      "metadata": {
        "id": "VK8i3VvG-Bg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNR(nn.Module):\n",
        "    \"\"\"\n",
        "    Recurrent DQN with an lstm cell.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, mlp_units, num_actions, activation_fn):\n",
        "        super(DQNR, self).__init__()\n",
        "        self.encoder = MLP(in_features, mlp_units, activation_fn)\n",
        "        self.lstm = nn.LSTMCell(\n",
        "            input_size=self.encoder.out_features, hidden_size=self.encoder.out_features\n",
        "        )\n",
        "        self.state = None\n",
        "        self.q_net = Q_Net(self.encoder.out_features, num_actions)\n",
        "\n",
        "    def get_state_len(self):\n",
        "        return 2 * self.lstm.hidden_size\n",
        "\n",
        "    def _state_reshape_in(self, batch_size, n_agents):\n",
        "        \"\"\"\n",
        "        Reshapes the state of shape\n",
        "            (batch_size, n_agents, self.get_state_len())\n",
        "        to shape\n",
        "            (2, batch_size * n_agents, hidden_size).\n",
        "\n",
        "        :param batch_size: the batch size\n",
        "        :param n_agents: the number of agents\n",
        "        \"\"\"\n",
        "        self.state = (\n",
        "            self.state.reshape(\n",
        "                batch_size * n_agents,\n",
        "                2,\n",
        "                self.lstm.hidden_size,\n",
        "            )\n",
        "            .transpose(0, 1)\n",
        "            .contiguous()\n",
        "        )\n",
        "\n",
        "    def _state_reshape_out(self, batch_size, n_agents):\n",
        "        \"\"\"\n",
        "        Reshapes the state of shape\n",
        "            (2, batch_size * n_agents, hidden_size)\n",
        "        to shape\n",
        "            (batch_size, n_agents, self.get_state_len()).\n",
        "\n",
        "        :param batch_size: the batch size\n",
        "        :param n_agents: the number of agents\n",
        "        \"\"\"\n",
        "        self.state = self.state.transpose(0, 1).reshape(batch_size, n_agents, -1)\n",
        "\n",
        "    def _lstm_forward(self, x, reshape_state=True):\n",
        "        \"\"\"\n",
        "        A single lstm forward pass\n",
        "\n",
        "        :param x: Cell input\n",
        "        :param reshape_state: reshape the state to and from (batch_size, n_agents, -1)\n",
        "        \"\"\"\n",
        "        batch_size, n_agents, feature_dim = x.shape\n",
        "        # combine agent and batch dimension\n",
        "        x = x.view(batch_size * n_agents, -1)\n",
        "\n",
        "        if self.state is None:\n",
        "            lstm_hidden_state, lstm_cell_state = self.lstm(x)\n",
        "        else:\n",
        "            if reshape_state:\n",
        "                self._state_reshape_in(batch_size, n_agents)\n",
        "            lstm_hidden_state, lstm_cell_state = self.lstm(\n",
        "                x, (self.state[0], self.state[1])\n",
        "            )\n",
        "\n",
        "        self.state = torch.stack((lstm_hidden_state, lstm_cell_state))\n",
        "        x = lstm_hidden_state\n",
        "\n",
        "        # undo combine\n",
        "        x = x.view(batch_size, n_agents, -1)\n",
        "        if reshape_state:\n",
        "            self._state_reshape_out(batch_size, n_agents)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        h = self.encoder(x)\n",
        "        h = self._lstm_forward(h)\n",
        "        return self.q_net(h)\n",
        "\n",
        "\n",
        "class CommNet(DQNR):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features,\n",
        "        mlp_units,\n",
        "        num_actions,\n",
        "        comm_rounds,\n",
        "        activation_fn,\n",
        "    ):\n",
        "        super().__init__(in_features, mlp_units, num_actions, activation_fn)\n",
        "        assert comm_rounds >= 0\n",
        "        self.comm_rounds = comm_rounds\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        batch_size, n_agents, feature_dim = x.shape\n",
        "        h = self.encoder(x)\n",
        "\n",
        "        # manually reshape state\n",
        "        if self.state is not None:\n",
        "            self._state_reshape_in(batch_size, n_agents)\n",
        "\n",
        "        h = self._lstm_forward(h, reshape_state=False)\n",
        "\n",
        "        # explicitly exclude self-communication from mask\n",
        "        mask = mask * ~torch.eye(n_agents, dtype=bool, device=x.device).unsqueeze(0)\n",
        "\n",
        "        for _ in range(self.comm_rounds):\n",
        "            # combine hidden state h according to mask\n",
        "            # first add up hidden states according to mask\n",
        "            #    h has dimensions (batch, agents, features)\n",
        "            #    and mask has dimensions (batch, agents, neighbors)\n",
        "            #    => we have to transpose the mask to aggregate over all neighbors\n",
        "            c = torch.bmm(h.transpose(1, 2), mask.transpose(1, 2)).transpose(1, 2)\n",
        "            # then normalize according to number of neighbors per agent\n",
        "            c = c / torch.clamp(mask.sum(dim=-1).unsqueeze(-1), min=1)\n",
        "\n",
        "            # skip connection for hidden state and communication\n",
        "            h = h + c\n",
        "            # use new hidden state\n",
        "            self.state[0] = h.view(batch_size * n_agents, -1)\n",
        "\n",
        "            # pass through forward module\n",
        "            h = self._lstm_forward(h, reshape_state=False)\n",
        "\n",
        "        # manually reshape state in the end\n",
        "        self._state_reshape_out(batch_size, n_agents)\n",
        "        return self.q_net(h)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IpEpQrpz-Ejb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## State Aggregation\n",
        "\n",
        "### SUM"
      ],
      "metadata": {
        "id": "AGSMkNTI-KIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleAggregation(nn.Module):\n",
        "    def __init__(self, agg: str, mask_eye: bool) -> None:\n",
        "        super().__init__()\n",
        "        self.agg = agg\n",
        "        assert self.agg == \"mean\" or self.agg == \"sum\"\n",
        "        self.mask_eye = mask_eye\n",
        "\n",
        "    def forward(self, node_features, node_adjacency):\n",
        "        if self.mask_eye:\n",
        "            node_adjacency = node_adjacency * ~(\n",
        "                torch.eye(\n",
        "                    node_adjacency.shape[1],\n",
        "                    node_adjacency.shape[1],\n",
        "                    device=node_adjacency.device,\n",
        "                )\n",
        "                .repeat(node_adjacency.shape[0], 1, 1)\n",
        "                .bool()\n",
        "            )\n",
        "        feature_sum = torch.bmm(node_adjacency, node_features)\n",
        "        if self.agg == \"sum\":\n",
        "            return feature_sum\n",
        "        if self.agg == \"mean\":\n",
        "            num_neighbors = torch.clamp(node_adjacency.sum(dim=-1), min=1).unsqueeze(-1)\n",
        "            return feature_sum / num_neighbors\n"
      ],
      "metadata": {
        "id": "PCj3k7NC-NsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GCN\n",
        "GCN is a graph convolutional operator that handles Message Passing phase within the GNN. Implementation is available at [GCN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv) within the pytorch_geometric library that specializes on GNNs.\n",
        "\n",
        "GCN is based on the spectral approximations of convolutional"
      ],
      "metadata": {
        "id": "7S-b66SX-TyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NetMon"
      ],
      "metadata": {
        "id": "ojoJfBcX-Wrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download files from our Git repository\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NVIUhKxST-5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting Parameters\n",
        "We ran multiple sweeps on Weights & Biases to provide an overview of the parameter importance and correlations. Because of the time constraints for the project, we were not able to collect all the data we would like but regardless the results are reasonably clear. We selected the BEST models based on the gathered reward. Other interesting metrics would be spr (the mean ratio of the lenght of the path taken and the true shortest path) or throughput (mean ratio of the planes that reached the target during the episode).\n",
        "\n",
        "We ran two sweeps, one for only CommNet settings (first picture), the other sweep for comparing DQN and DGN (second picture). Both sweeps ran for 75k steps, with CommNet being significantly faster yet inferior architecture and Sum being the superior AND faster aggregation method.\n",
        "\n",
        "## Common Parameters in the Sweep\n",
        " - mini_batch_size : number of sampled experiences from the replay buffer\n",
        " - epsilon_decay : decay factor of epsilon in the EpsilonGreedy policy\n",
        " - agg_type : method used during GNN message aggregation phase\n",
        " - gamma: multiplicative factor in Q-Learning\n",
        "\n",
        "## CommNet specific\n",
        " - comm_round: number of information passing round\n",
        "\n",
        "## DQN, DGN specific\n",
        " - num_heads: number of attention heads\n",
        "\n"
      ],
      "metadata": {
        "id": "1Ei12632gg3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Advice for Parameter Selection\n",
        "Below we give advice how to fine tune the hyperparameters and then we show results from the sweeps on wandb. 35 sweeps were performed for the two settings.\n",
        "## CommNet settings\n",
        "It is apparent that selecting lower number of com_rounds as well as increasing the mini_batch_size improves the model greatly. Although the epsilon_update_frequency does not show as important, it could be due to the narrow range we selected for the sweep. We advice to set this parameter to about 70 for 75k steps and gradually increase this number with increasing number of steps. A good rule of thumb is that after the training the epsilon value should be around 0.01, where there is a hard line so that if epsilon dips below 0.01 it is reset to 0.01.\n",
        "## DQN, DGN settings\n",
        "Gamma seemed to be the most important hyperparameter in this setting, which is not all that surprising because of the architecture that goes almost directly into the Q_Net. We saw consistent improvement in the gathered reward with the gamma parameter being set lower (to around 0.92-0,95) for 75k steps. Epsilon update frequency shows here as a very important hyperparameter with a strong negative correlation. This means that epsilon should be updated more often, which suggests that the models are able to learn to navigate the environment quickly. The number of attention heads and attention layers shows a positive correlation with the reward.\n",
        "\n",
        "## Aggregation Type\n",
        "In both sweeps the aggregation type didn't hold much importance. In the case of CommNet the suggestion would be to use GCN on basis that GCN had positive correlation and SUM had negative correlation with the generated reward.\n",
        "On the other hand, in the case of DQN and DGN, SUM seemed to perform better. From our own experience from training the models for more steps, we would reccomend using SUM as it is faster and the results are comparable."
      ],
      "metadata": {
        "id": "OhkNKLaYlGRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WANDB sweep for CommNet\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/commnet.png?raw=true)"
      ],
      "metadata": {
        "id": "e1gjgCZZzhzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters importance for CommNet\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/commnet_parameters.png?raw=true)"
      ],
      "metadata": {
        "id": "ghDCmQ55xYYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WANDB sweep for DQN vs. DGN\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/dgn_dqn.png?raw=true)"
      ],
      "metadata": {
        "id": "TO3iriopz9P7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HYPERPARAMETERS IMPORTANCE FOR DGN and DQN\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/dgn_dqn_parameters.png?raw=true)"
      ],
      "metadata": {
        "id": "ca4DwESCxrjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our Results\n",
        "\n",
        "We trained multiple models with various settings but after performing the sweeps on Weights and Biases, our mean reward nearly doubled and so did the throughput of planes into targets. What's more interesting is that we observed a clean positive correlation between reward and throughput, meaning that with higher mean reward achieved the number of planes that landed was also higher. This effectively means that the agents did not find a loophole in the reward system, which is very pleasant and we are very intrigued by this result. These claims deserve more backing and rigorous testing as well as the before mentioned sweeps of the hyperparameter space. Unfortunatelly, we didn't have enough time during this project to give attention to all these things that arguably deserve it.\n",
        "\n",
        "## Our Selected Best Performing Models\n",
        "\n",
        "We present the top two models we have selected based on mean reward. Note that higher mean reward very likely implies higher throuput and smaller shortest path ratio. We also added a third model for comparison to include every architecture we used.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S8nU4WQLwJI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Parameters               | Model 1          | Model 2          | Model 3        |\n",
        "|-------------------------|------------------------|------------------------|------------------------|\n",
        "| model_type              | comm_net               | dqn                    | dgn                    |\n",
        "| iterations              | 6                      | 6                      | 8                      |\n",
        "| agg_type                | gcn                    | gcn                    | sum                    |\n",
        "| att_layers              | -                     | -                      | 6                      |\n",
        "| num_heads            | - |        -             | 12|\n",
        "| kv_values               | -                     | -                     | 16                     |\n",
        "| comm_rounds             | 4                      | -                      | -                      |\n",
        "| epsilon_update_freq     | 70                     | 90                     | 70                     |\n",
        "| total_steps             | 75000                  | 75000                  | 75000                  |\n",
        "| step_before_train       | 15000                  | 15000                  | 15000                  |\n",
        "| step_between_train      | 10                     | 10                     | 5                      |\n",
        "| sequence_length         | 16                     | 16                     | 16                     |\n",
        "| gamma                   | 0.93                   | 0.91                   | 0.91                   |\n",
        "| mini_batch_size         | 32                     | 64                     | 32                     |\n",
        "|-------------------------|------------------------|------------------------|------------------------|\n",
        "|Results|\n",
        "|-------------------------|------------------------|------------------------|------------------------|\n",
        "| delays_mean             | 20.7              | **17.4**              | 28.6              |\n",
        "| delays_arrived_mean     | 21.8             | **18**              | 29              |\n",
        "| spr_mean                | 3.7               | **3**               | 5              |\n",
        "| looped_mean             | 0.41               | **0.05**               | 1.3               |\n",
        "| throughput_mean         | 0.42                  | **0.51**               | 0.29               |\n",
        "| dropped_mean            | 0.0                    | 0.0                    | 0.0                    |\n",
        "| blocked_mean            | 0.001               | 0.002               | 0.001               |\n",
        "| total_edge_load_mean    | 5               | 5               | 5               |\n",
        "| occupied_edges_mean     | 9.4              | 9.4               | 9.3               |\n",
        "| planes_on_edges_mean    | 9.99               | 9.99               | 9.99               |\n",
        "| total_plane_size_mean   | 5.0                    | 5.0                    | 5.0                    |\n",
        "| plane_sizes_mean        | 0.5                    | 0.5                    | 0.5                    |\n",
        "| plane_distances_mean    | 9.3                | 9.6               | 9.1               |\n",
        "| reward_mean             | 3.5              | **3.9**               | 2.7               |\n"
      ],
      "metadata": {
        "id": "eNkykOCkxXlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Heatmaps\n",
        "Heatmaps show the edge and node usage by the agents. Additional pictures could be found in the /pictures /evaluation_pictures files or self generated by the code. After each evaluation phase the code automatically saves many interesting pictures. These were taken from the original implementation and slightly beatufied."
      ],
      "metadata": {
        "id": "K0VfxzH_jn4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMMNET\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/evaluation_pictures/heatmapcommnetmask.png?raw=true)"
      ],
      "metadata": {
        "id": "8GQ6b3ibeK6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DQN\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/evaluation_pictures/heatmapdqnmask.png?raw=true)"
      ],
      "metadata": {
        "id": "eNq4qi-Pjbwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DGN\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/evaluation_pictures/heatmapdgnmask.png?raw=true)"
      ],
      "metadata": {
        "id": "xkXvwNuhkwSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Shortest Path Ratio\n",
        "These plots show the relation between the number of steps in shortest path to target and the number of steps the agent took to reach it. The linear line in the middle shows the \"lower bound\" but the number of steps need to scaled by the planes speed to make this interpretation valid. Here it juste serves as a comparison for the stability of the model.\n",
        "\n",
        "We can make an interesting observation that with the growing number of steps needed to reach the target, the number of steps the agents take to reach it grows almost linearly with some deviations. The biggest instability is in the DGN model, while DQN appears to be exceptionally stable. In the case of CommNet, the model behaves even better for longer paths, whis is simply lovely."
      ],
      "metadata": {
        "id": "MMiw4WCyk_W9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMMNET\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/evaluation_pictures/sprcommnetmask.png?raw=true)"
      ],
      "metadata": {
        "id": "-qaQ7fj_mNZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DQN\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/evaluation_pictures/sprdqnmask.png?raw=true)"
      ],
      "metadata": {
        "id": "E3XestfLmXa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DGN\n",
        "![](https://github.com/Strojove-uceni/2024-final-letadylka-prochazka-belohlavek/blob/main/pictures/evaluation_pictures/sprdgnmask.png?raw=true)"
      ],
      "metadata": {
        "id": "K-eaJH9tmbec"
      }
    }
  ]
}